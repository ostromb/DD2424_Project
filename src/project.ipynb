{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2424 Project in Deep Learning in Data Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tqdm\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filename = '../Dataset/Training/edgar.txt'\n",
    "book_data = np.array(load_data(training_data_filename))\n",
    "book_chars = np.unique(book_data)\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "m = 100\n",
    "eta = 0.1\n",
    "seq_length = 25\n",
    "sig = 0.01\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m, k, eta, seq_length, sig)\n",
    "h0 = np.zeros((m, 1))\n",
    "max_iter = 200000\n",
    "epochs = 10\n",
    "smoothloss_list = []\n",
    "loss_list = []\n",
    "iteration = 0\n",
    "smoothloss = 0\n",
    "sentences = []\n",
    "for i in range(epochs):\n",
    "    rnn.hprev = np.zeros((m, 1))\n",
    "    for e in range(0, book_data.shape[0]-seq_length-1, seq_length):\n",
    "        X_chars = book_data[e:e+seq_length]\n",
    "        Y_chars = book_data[e+1:e+seq_length+1]\n",
    "        X = one_hot_encoding(X_chars, char_to_ind, k)\n",
    "        Y = one_hot_encoding(Y_chars, char_to_ind, k)\n",
    "        loss = rnn.adagrad(X, Y, h0, iteration)\n",
    "        if smoothloss == 0:\n",
    "            smoothloss = loss\n",
    "        smoothloss = 0.999*smoothloss + 0.001*loss\n",
    "     \n",
    "        if iteration % 10000 == 1:\n",
    "            print('Iteration: {}, Loss: {} '.format(iteration, smoothloss))\n",
    "            y = rnn.synthetize(rnn.hprev, X[:, 0], 200)\n",
    "            sentence = one_hot_decoding(y, ind_to_char)\n",
    "            print(sentence + \"\\n\")\n",
    "            #sentences.append(sentence)\n",
    "            smoothloss_list.append(smoothloss)\n",
    "            loss_list.append(loss)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration>max_iter:\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(training_data_filename,encoding='utf-8-sig',mode='r') as file:\n",
    "    validation_text = file.read()\n",
    "start_char = \"T\"\n",
    "start_char_onehot = one_hot_encoding(start_char, char_to_ind, k)\n",
    "generated_text_vanilla_onehot = rnn.synthetize(rnn.hprev, start_char_onehot, 1000)\n",
    "generated_text_vanilla = start_char + one_hot_decoding(generated_text_vanilla_onehot, ind_to_char)\n",
    "print(generated_text_vanilla)\n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "nmax = 4\n",
    "fraction_correct_words, bleu_score = measure_bleu(text_generated=generated_text_vanilla, text_val=validation_text, n_max=nmax)\n",
    "repetition_score = measure_diversity(text_generated=generated_text_vanilla, n_max=nmax)\n",
    "print(\"\\n loss function\", loss_list)\n",
    "print(\"\\n fraction of correctly spelled words: {} \\n Bleu score: {} \\n Repetition score: {}\".format(fraction_correct_words, bleu_score, repetition_score))\n",
    "\n",
    "\n",
    "\n",
    "fig = px.line(smoothloss_list, title='Smoothed loss over epochs', width=600)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_yaxes(title_text=\"smoothed loss\")\n",
    "fig.update_xaxes(title_text=\"iteration step, in multiples of 10k\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "encoding = \"utf8\" #\"utf8\"\n",
    "\n",
    "training_data_filename = '../Dataset/Training/edgar.txt'\n",
    "book_data, words = np.array(load_data(training_data_filename,remove_footnotes=False,word_level=False, encoding=encoding))\n",
    "#book_data, words = load_data1(training_data_filename, remove_footnotes=False, word_level=False, encoding=encoding)\n",
    "#book_data = book_data[:10000]\n",
    "\n",
    "\n",
    "with open(training_data_filename,encoding=encoding,mode='r') as f:\n",
    "  words = f.read().split()\n",
    "with open(training_data_filename,encoding=encoding,mode='r') as file:\n",
    "    validation_text = file.read().replace('\\n', ' ')\n",
    "\n",
    "data_to_use = words\n",
    "\n",
    "book_chars = np.unique(data_to_use)\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "#print(len(ind_to_char))\n",
    "vocab_size = len(book_chars)\n",
    "embedding_dim = 256 \n",
    "print(len(words))\n",
    "print(len(book_data))\n",
    "book_data_ind = np.array([char_to_ind[c] for c in data_to_use])\n",
    "print(len(book_data_ind))\n",
    "#print(ind_to_char)\n",
    "print(book_chars)\n",
    "def create_batches(batch_size, seq_length):\n",
    "\n",
    "    # Split data into sequences\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(book_data_ind)\n",
    "    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "    # Split data into X, Y\n",
    "    dataset = sequences.map(split_input_target)\n",
    "\n",
    "    # This organizes the data into groups of sequences. batch_size denotes the number of sequences in a batch, and seq_length denotes the number of characters in a sequence.\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BPE Tokenization\"\"\"\n",
    "\n",
    "def compute_pair_count(splits, word_count):\n",
    "    pair_count = {}\n",
    "    for word, count in word_count.items():\n",
    "        split = splits[word]\n",
    "      \n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            if pair in pair_count:\n",
    "                pair_count[pair] += count\n",
    "            else:\n",
    "                pair_count[pair] = count\n",
    "    return pair_count\n",
    "\n",
    "def merge_pair(a, b, splits,word_count):\n",
    "    for word in word_count:\n",
    "     \n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "    \n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        splits[word] = split\n",
    "   \n",
    "    return splits\n",
    "\n",
    "def BPE(text, vocab_size,word_count):\n",
    "    merges = {}\n",
    "    book_chars = np.unique(text)\n",
    "    splits =  {word: [c for c in word] for word in word_count.keys()}\n",
    "    while len(book_chars) < vocab_size:\n",
    "        pair_count = compute_pair_count(splits, word_count)\n",
    "        best_pair = \"\"\n",
    "        max_count = None\n",
    "        for pair, count in pair_count.items():\n",
    "            if max_count is None or max_count < count:\n",
    "                best_pair = pair\n",
    "                max_count = count\n",
    "        splits = merge_pair(*best_pair, splits, word_count)\n",
    "        merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "        book_chars = np.append(book_chars,best_pair[0] + best_pair[1])\n",
    "    return splits, merges, book_chars\n",
    "\n",
    "def tokenize(words, merges):\n",
    "    word_count = {}\n",
    "    for word in words:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "    \n",
    "    splits =  [[c for c in word] for word in word_count.keys()]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data for BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'à'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m split_ind1 \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(N\u001b[39m*\u001b[39m\u001b[39m0.8\u001b[39m)\n\u001b[0;32m     43\u001b[0m split_ind2 \u001b[39m=\u001b[39m split_ind1 \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(N\u001b[39m*\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m data_ind_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([char_to_ind[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m tokenized_text[\u001b[39m0\u001b[39m:split_ind1]])\n\u001b[0;32m     46\u001b[0m data_ind_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([char_to_ind[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m tokenized_text[split_ind1:split_ind2]])\n\u001b[0;32m     47\u001b[0m data_ind_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([char_to_ind[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m tokenized_text[split_ind2:]])\n",
      "Cell \u001b[1;32mIn[12], line 45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m split_ind1 \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(N\u001b[39m*\u001b[39m\u001b[39m0.8\u001b[39m)\n\u001b[0;32m     43\u001b[0m split_ind2 \u001b[39m=\u001b[39m split_ind1 \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(N\u001b[39m*\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m data_ind_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([char_to_ind[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m tokenized_text[\u001b[39m0\u001b[39m:split_ind1]])\n\u001b[0;32m     46\u001b[0m data_ind_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([char_to_ind[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m tokenized_text[split_ind1:split_ind2]])\n\u001b[0;32m     47\u001b[0m data_ind_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([char_to_ind[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m tokenized_text[split_ind2:]])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'à'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vol1_filename = '../Dataset/Training/edgar1.txt'\n",
    "vol2_filename = '../Dataset/Training/edgar2.txt'\n",
    "vol3_filename = '../Dataset/Training/edgar3.txt'\n",
    "vol4_filename = '../Dataset/Training/edgar4.txt'\n",
    "vol5_filename = '../Dataset/Training/edgar5.txt'\n",
    "\n",
    "book_data1, words1 = load_data1(vol1_filename,remove_footnotes=False,word_level=False)\n",
    "book_data3, words3 = load_data1(vol3_filename,remove_footnotes=False,word_level=False)\n",
    "book_data4, words4 = load_data1(vol4_filename,remove_footnotes=False,word_level=False)\n",
    "book_data5, words5 = load_data1(vol5_filename,remove_footnotes=False,word_level=False)\n",
    "val_book_data, val_words = load_data1(vol2_filename,remove_footnotes=False,word_level=False)\n",
    "\n",
    "#all_training_words = np.concatenate((words1, words3, words4))\n",
    "#all_training_data = np.concatenate((book_data1, book_data3, book_data4))\n",
    "all_training_words = words1\n",
    "all_training_data = book_data1\n",
    "all_words = np.concatenate((words1, val_words, words3, words4))\n",
    "all_data = np.concatenate((book_data1, book_data4, book_data3, book_data4))\n",
    "\n",
    "\n",
    "total_validaton_text = \"\"\n",
    "for i in range(1,6):\n",
    "    with open('../Dataset/Training/edgar{}.txt'.format(i), \"r\", encoding=\"utf-8-sig\", ) as f:\n",
    "        total_validaton_text += f.read()\n",
    "\n",
    "\n",
    "word_count = {}\n",
    "for word in all_words:\n",
    "    if word in word_count:\n",
    "        word_count[word] += 1\n",
    "    else:\n",
    "        word_count[word] = 1\n",
    "\n",
    "book_chars = np.unique(all_data)\n",
    "splits, merges, book_chars = BPE(all_training_data, 200, word_count)\n",
    "tokenized_text = tokenize(all_words, merges)\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "\n",
    "N = len(tokenized_text)\n",
    "split_ind1 = int(N*0.8)\n",
    "split_ind2 = split_ind1 + int(N*0.1)\n",
    "\n",
    "data_ind_train = np.array([char_to_ind[c] for c in tokenized_text[0:split_ind1]])\n",
    "data_ind_val = np.array([char_to_ind[c] for c in tokenized_text[split_ind1:split_ind2]])\n",
    "data_ind_test = np.array([char_to_ind[c] for c in tokenized_text[split_ind2:]])\n",
    "\n",
    "k = book_chars.shape[0]\n",
    "embedding_dim = 256\n",
    "vocab_size = len(book_chars)\n",
    "\n",
    "def create_batches(data_ind, batch_size, seq_length):\n",
    "    # Split data into sequences\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(data_ind)\n",
    "    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "    # Split data into X, Y\n",
    "    dataset = sequences.map(split_input_target)\n",
    "\n",
    "    # This organizes the data into groups of sequences. batch_size denotes the number of sequences in a batch, and seq_length denotes the number of characters in a sequence.\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM and BPE functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim,num_ns):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots\n",
    "  \n",
    "\n",
    "def train_word2Vec(dataset,vocab_size, num_ns, n_epochs):\n",
    "\n",
    "    word2vec = Word2Vec(vocab_size, embedding_dim,num_ns)\n",
    "    word2vec.compile(optimizer='adam',\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "    word2vec.fit(dataset, epochs=n_epochs, callbacks=[tensorboard_callback])\n",
    "    return word2vec\n",
    "\n",
    "\n",
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
    "\n",
    "def build_model(rnn_units, batch_size,weights=None):\n",
    "    \n",
    "    m = tf.keras.Sequential()\n",
    "    if weights is not None:\n",
    "        m.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None],embeddings_initializer=tf.keras.initializers.Constant(weights)))\n",
    "    else:\n",
    "       m.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    m.add(tf.keras.layers.BatchNormalization())\n",
    "    m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    m.add(tf.keras.layers.Dense(vocab_size))\n",
    "    #m.summary()\n",
    "    return m\n",
    "\n",
    "\n",
    "def train_model(dataset_train, dataset_val, eta, rnn_units, batch_size, output_filename,n_epochs,weights=None):\n",
    "    m = build_model(rnn_units, batch_size,weights)\n",
    "    for i_ex, t_ex in dataset.take(1):\n",
    "        example_pred = m(i_ex)  # this step builds the model\n",
    "\n",
    "    # Specify update rule and compile model\n",
    "    adam_opt = tf.keras.optimizers.Adam(learning_rate=eta)\n",
    "    loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    m.compile(optimizer=adam_opt, loss=loss_func)\n",
    "\n",
    "    # Configure checkpoints\n",
    "    current_dir_path = os.getcwd()\n",
    "    checkpoint_dir = os.path.join(os.path.join(os.path.join(current_dir_path, \"tmp\"), output_filename), \"training_checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch:02d}.hdf5')\n",
    "    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True, save_best_onl =True)\n",
    "\n",
    "    # train\n",
    "    if dataset_val is not None:\n",
    "      history = m.fit(x=dataset, validation_data=dataset_val, epochs=n_epochs, callbacks=[checkpoint_callback])\n",
    "    else:\n",
    "      history = m.fit(x=dataset, epochs=n_epochs, callbacks=[checkpoint_callback])\n",
    "    return m, history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "modelpathname = \"v6_2\"\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "output_str = \"batch_size | seq_len | eta | loss  \\n\"\n",
    "\n",
    "batch_sizes = [25, 75, 125]\n",
    "seq_lengths = [25, 75, 125]\n",
    "learningrates = [0.1, 0.01, 0.001]\n",
    "count = 0\n",
    "for batch_size in batch_sizes:\n",
    "    for seq_length in seq_lengths:\n",
    "        dataset = create_batches(batch_size, seq_length)\n",
    "        for eta in learningrates:\n",
    "            print(count/27)\n",
    "            model, history = train_model(dataset, eta, rnn_units, batch_size, modelpathname, n_epochs)\n",
    "            final_loss = history.history['loss'][-1]\n",
    "            # generate text\n",
    "            m = build_model(rnn_units, 1)\n",
    "            m.load_weights(checkpoint_dir + \"/\" + \"ckpt_0{}.hdf5\".format(n_epochs))\n",
    "            m.build(tf.TensorShape([1, None]))\n",
    "            #gen_text = generate_text(model=m, start_string=\"The \", text_size=1000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=1.0, p=None)\n",
    "            # Measure performance\n",
    "            #frac_corr_words, bleu2 = measure_bleu(gen_text, validation_text, 2)\n",
    "            output_str += \"{}      {}      {}      {}       \\n\".format(batch_size, seq_length, eta, final_loss)\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "\n",
    "print(output_str)  \n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "37/37 [==============================] - 17s 378ms/step - loss: 4.2359 - val_loss: 4.7520\n",
      "Epoch 2/2\n",
      "37/37 [==============================] - 14s 378ms/step - loss: 3.4914 - val_loss: 4.4701\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelpathname = 'test'\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "# Number of RNN units.\n",
    "rnn_units = 200\n",
    "n_epochs = 2\n",
    "eta = 0.01 # Best = 0.01\n",
    "batch_size = 25 # Best = 25\n",
    "seq_length = 100 # Best = 125\n",
    "\n",
    "dataset_train = create_batches(data_ind_train, batch_size, seq_length)\n",
    "dataset_val = create_batches(data_ind_val, batch_size, seq_length)\n",
    "\n",
    "model, history = train_model(dataset, dataset_val, eta, rnn_units, batch_size, modelpathname, n_epochs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text, evauate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_02.hdf5\n",
      "hej the renly. I s y s stis  asher\n",
      " nalo a ouin. murn. ssims, corat. “whe  seing\n",
      "  frin ha dy, vic\n",
      " bep w—n. calce des vathe dis thnt—hur uhe, rell,\n",
      " wed\n",
      " ol?.\n",
      " gor, il sed c. de,\n",
      " laute, atter,\n",
      " ray muron. para\n",
      " uyth aetting fried sear by stl \n",
      " He. Ow\n",
      "\n",
      " im a dow., stid hatc fa red’ell seve. heor sal. aenthe dey,,—st. sces—th tior su,. sataring prop, le. Fro f Mh n, ch— d yd houl a,,,med\n",
      " beat real, coche, pok—se th P,s fe o wher ninu. pred\n",
      " part ge pit\n",
      "\n",
      "hon, dei, with, dis as fod ad,\n",
      " vo m walp lar, wi, toar\n",
      ", exp fing—t inried, yp be\n",
      " carm s.L soa\n",
      " “I siro. chin—s, shar h\n",
      " murn, sting\n",
      " “ lw th—w, caming dagh\n",
      " There he ad see, corat flan ding\n",
      "as lc,,\n",
      " li, suce in lan,\n",
      "\n",
      "am, bare acts,—ir. adly soment. af” lop, mig  thez se agation, licdation ear,\n",
      " bird\n",
      " het\n",
      " ast. loud. ad, sub. cadn,\n",
      " beat ey nane assan,.\n",
      " “i pop. sating mear su. am, stron, bac carse poing\n",
      " tiou  oll,\n",
      " fod fortbe ishe, imat blin— W— m. ad t vi re parc, sarit ence  fe on,.she ds, gard seat\n",
      " rig re be. plut, inceou, exurs,\n",
      " cinn, band s dear\n",
      ". dalk gith les ady marle exarth haous, aly. ræ\n",
      " wyino be,\n",
      "ti\n",
      " spo,\n",
      "—f th,. undhe an, cr cont. fak, end\n",
      " Ban mat manit, thand,\n",
      " beN chat, lo, ashe  rias mol, scf\n",
      " malch  va., ob, toer onl no, cithe.erl, whoun, core,\n",
      "aym pond hap. chg shan\n",
      " ese ger be mes in, gre\n",
      " agas su fap, thou\n",
      "— s\n",
      " t as  seatds “a d. gomv. cars,\n",
      "ort slo. alom, anu\n",
      " erts ri p\n",
      " cond as les wes, aar.\n",
      " ginter\n",
      " onn con wst, decoed coo ly, ner. conro he. ani, parim\n",
      "\n",
      " conid d blb em\n",
      " pl\n",
      " mido\n",
      " boc,. aus fanu —lc. brich  “w g wen v cating—o b canlt allous\n",
      " concehe. conr n cke. “Thea nean, lhe c Od! meat? yar\n",
      " freith ma\n",
      "the renly. I s y s stis  asher\n",
      " nalo a ouin. murn. ssims, corat. “whe  seing\n",
      "  frin ha dy, vic\n",
      " bep w—n. calce des vathe dis thnt—hur uhe, rell,\n",
      " wed\n",
      " ol?.\n",
      " gor, il sed c. de,\n",
      " laute, atter,\n",
      " ray muron. para\n",
      " uyth aetting fried sear by stl \n",
      " He. Ow\n",
      "\n",
      " im a dow., stid hatc fa red’ell seve. heor sal. aenthe dey,,—st. sces—th tior su,. sataring prop, le. Fro f Mh n, ch— d yd houl a,,,med\n",
      " beat real, coche, pok—se th P,s fe o wher ninu. pred\n",
      " part ge pit\n",
      "\n",
      "hon, dei, with, dis as fod ad,\n",
      " vo m walp lar, wi, toar\n",
      ", exp fing—t inried, yp be\n",
      " carm s.L soa\n",
      " “I siro. chin—s, shar h\n",
      " murn, sting\n",
      " “ lw th—w, caming dagh\n",
      " There he ad see, corat flan ding\n",
      "as lc,,\n",
      " li, suce in lan,\n",
      "\n",
      "am, bare acts,—ir. adly soment. af” lop, mig  thez se agation, licdation ear,\n",
      " bird\n",
      " het\n",
      " ast. loud. ad, sub. cadn,\n",
      " beat ey nane assan,.\n",
      " “i pop. sating mear su. am, stron, bac carse poing\n",
      " tiou  oll,\n",
      " fod fortbe ishe, imat blin— W— m. ad t vi re parc, sarit ence  fe on,.she ds, gard seat\n",
      " rig re be. plut, inceou, exurs,\n",
      " cinn, band s dear\n",
      ". dalk gith les ady marle exarth haous, aly. ræ\n",
      " wyino be,\n",
      "ti\n",
      " spo,\n",
      "—f th,. undhe an, cr cont. fak, end\n",
      " Ban mat manit, thand,\n",
      " beN chat, lo, ashe  rias mol, scf\n",
      " malch  va., ob, toer onl no, cithe.erl, whoun, core,\n",
      "aym pond hap. chg shan\n",
      " ese ger be mes in, gre\n",
      " agas su fap, thou\n",
      "— s\n",
      " t as  seatds “a d. gomv. cars,\n",
      "ort slo. alom, anu\n",
      " erts ri p\n",
      " cond as les wes, aar.\n",
      " ginter\n",
      " onn con wst, decoed coo ly, ner. conro he. ani, parim\n",
      "\n",
      " conid d blb em\n",
      " pl\n",
      " mido\n",
      " boc,. aus fanu —lc. brich  “w g wen v cating—o b canlt allous\n",
      " concehe. conr n cke. “Thea nean, lhe c Od! meat? yar\n",
      " freith ma\n",
      " ------------ \n",
      " loss function: [4.235851287841797, 3.491445779800415] \n",
      " fraction of correctly spelled words: 0.20987654320987653 \n",
      " Bleu score2: 0.025490637096729087, Repetition score2: 0.01694915254237288 \n",
      " Bleu score4: 0.0, Repetition score4: 0.01694915254237288 \n",
      " settings: batch_size, seq_length, eta, rnn_units, n_epochs = 25, 100, 0.01, 200, 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "plotbool = False\n",
    "outputfile_bool = False\n",
    "\n",
    "# Find latest checkpoint file, because tf.train.latest_checkpoint(dir) doesn't work for some reason\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "    if e>latest_epoch:\n",
    "        latest_epoch = e\n",
    "        latest_checkpoint_file = file\n",
    "print(latest_checkpoint_file)\n",
    "#latest_checkpoint_file = \"ckpt_01.hdf5\"\n",
    "\n",
    "m = build_model(rnn_units, 1)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([1, None]))\n",
    "\n",
    "nucleus_probability = None\n",
    "temp = 0.9\n",
    "gen_text = generate_text1(model=m, start_string=\"the\", text_size=1000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=temp, p=nucleus_probability)\n",
    "gen_text = \"\".join(gen_text).replace(\"$\",\" \")\n",
    "print(\"hej\", gen_text)\n",
    "\n",
    "if plotbool:\n",
    "    d = get_n_grams(gen_text, 1)\n",
    "    fig = px.line(history.history['loss'], title='Loss over epochs', width=600)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_xaxes(title_text=\"Iteration step, in multiples of 10k\")\n",
    "    fig.update_yaxes(title_text=\"smoothed loss\")\n",
    "    fig.show()\n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=2)\n",
    "_, bleu_score4 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=4)\n",
    "repetition_score2 = measure_diversity(text_generated=gen_text, n_max=2)\n",
    "repetition_score4 = measure_diversity(text_generated=gen_text, n_max=4)\n",
    "\n",
    "output_str = gen_text + \"\\n ------------ \\n loss function: {} \\n fraction of correctly spelled words: {} \\n Bleu score2: {}, Repetition score2: {} \"\\\n",
    "    \"\\n Bleu score4: {}, Repetition score4: {} \".format(history.history['loss'], fraction_correct_words, bleu_score2, repetition_score2, bleu_score4, repetition_score4)\n",
    "output_str += \"\\n settings: batch_size, seq_length, eta, rnn_units, n_epochs = {}, {}, {}, {}, {}\".format(batch_size, seq_length, eta, rnn_units, n_epochs)\n",
    "print(output_str)\n",
    "\n",
    "\n",
    "# Create output file\n",
    "output_path = os.path.join(os.path.join(os.path.join(os.getcwd(), \"tmp\"), modelpathname), \"text_temp15.txt\")\n",
    "if outputfile_bool:\n",
    "    with open(output_path, \"w\",encoding=\"utf-8-sig\") as file:\n",
    "        file.write(output_str)\n",
    "\n",
    "#rint(gen_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpathname = 'v10_2'\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "# Number of RNN units.\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "eta = 0.01 # Best = 0.01\n",
    "batch_size = 50 # Best = 25\n",
    "seq_length = 50 # Best = 125\n",
    "num_ns = 4\n",
    "\n",
    "#Initalize the dataset that will be used for LSTM network\n",
    "dataset1 = create_batches(batch_size, seq_length)\n",
    "seq_list = []\n",
    "for batch in dataset1:\n",
    "   for seq in batch[0]:\n",
    "       seq_list.append(seq.numpy())\n",
    "print(len(seq_list))\n",
    "\n",
    "#Generate training data for word2vec using the sequences from the LSTM dataset\n",
    "targets, contexts, labels = generate_training_data(seq_list, 2, 4, vocab_size)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "#Train word2vec model and get the weights\n",
    "word2vec = train_word2Vec(dataset,vocab_size, num_ns, 20)\n",
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "\n",
    "#Train LSTM model\n",
    "model, history = train_model(dataset1, eta, rnn_units, batch_size, modelpathname, n_epochs, weights=weights)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)\n",
    "pd.DataFrame(weights).to_csv(\"weights_word2vec.csv\")\n",
    "word2vec.save_weights(\"word2vec_weights_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpathname = 'v12_2'\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "# Number of RNN units.\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "eta = 0.01 # Best = 0.01\n",
    "batch_size = 50 # Best = 25\n",
    "seq_length = 50 # Best = 125\n",
    "num_ns = 4\n",
    "dataset1 = create_batches(batch_size, seq_length)\n",
    "weights1 = pd.read_csv(\"weights_word2vec.csv\", index_col=0).to_numpy()\n",
    "\n",
    "model, history = train_model(dataset1, eta, rnn_units, batch_size, modelpathname, n_epochs, weights=weights1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotbool = False\n",
    "outputfile_bool = True\n",
    "\n",
    "# Find latest checkpoint file, because tf.train.latest_checkpoint(dir) doesn't work for some reason\n",
    "modelpathname = 'v12_2'\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "eta = 0.01 # Best = 0.01\n",
    "batch_size = 50 # Best = 25\n",
    "seq_length = 50 # Best = 125\n",
    "num_ns = 4\n",
    "#rint(os.path)3\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "    if e>latest_epoch:\n",
    "        latest_epoch = e\n",
    "        latest_checkpoint_file = file\n",
    "print(latest_checkpoint_file)\n",
    "#latest_checkpoint_file = \"ckpt_01.hdf5\"\n",
    "print(ind_to_char)\n",
    "m = build_model(rnn_units, 1)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([1, None]))\n",
    "\n",
    "nucleus_probability = None\n",
    "temp = 1.0\n",
    "gen_text = generate_text(model=m, start_string=\"the\", text_size=2000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=temp, p=nucleus_probability,word_level=True)\n",
    "\n",
    "\n",
    "if plotbool:\n",
    "    d = get_n_grams(gen_text, 1)\n",
    "    fig = px.line(history.history['loss'], title='Loss over epochs', width=600)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_xaxes(title_text=\"Iteration step, in multiples of 10k\")\n",
    "    fig.update_yaxes(title_text=\"smoothed loss\")\n",
    "    fig.show()\n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "# fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=2)\n",
    "# _, bleu_score4 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=4)\n",
    "# repetition_score2 = measure_diversity(text_generated=gen_text, n_max=2)\n",
    "# repetition_score4 = measure_diversity(text_generated=gen_text, n_max=4)\n",
    "\n",
    "# output_str = gen_text + \"\\n ------------ \\n loss function: {} \\n fraction of correctly spelled words: {} \\n Bleu score2: {}, Repetition score2: {} \"\\\n",
    "#     \"\\n Bleu score4: {}, Repetition score4: {} \".format(history.history['loss'], fraction_correct_words, bleu_score2, repetition_score2, bleu_score4, repetition_score4)\n",
    "# output_str += \"\\n settings: batch_size, seq_length, eta, rnn_units, n_epochs = {}, {}, {}, {}, {}\".format(batch_size, seq_length, eta, rnn_units, n_epochs)\n",
    "print(gen_text)\n",
    "\n",
    "\n",
    "#Create output file\n",
    "output_path = os.path.join(os.path.join(os.path.join(os.getcwd(), \"tmp\"), modelpathname), \"text_temp15.txt\")\n",
    "if outputfile_bool:\n",
    "    with open(output_path, \"w\",encoding=\"utf-8-sig\") as file:\n",
    "        file.write(gen_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BPE Tokenization\"\"\"\n",
    "\n",
    "def compute_pair_count(splits, word_count):\n",
    "    pair_count = {}\n",
    "    for word, count in word_count.items():\n",
    "        split = splits[word]\n",
    "      \n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            if pair in pair_count:\n",
    "                pair_count[pair] += count\n",
    "            else:\n",
    "                pair_count[pair] = count\n",
    "    return pair_count\n",
    "\n",
    "def merge_pair(a, b, splits,word_count):\n",
    "    for word in word_count:\n",
    "     \n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "    \n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        splits[word] = split\n",
    "   \n",
    "    return splits\n",
    "\n",
    "def BPE(text, vocab_size,word_count):\n",
    "    merges = {}\n",
    "    book_chars = np.unique(text)\n",
    "    splits =  {word: [c for c in word] for word in word_count.keys()}\n",
    "    while len(book_chars) < vocab_size:\n",
    "        pair_count = compute_pair_count(splits, word_count)\n",
    "        best_pair = \"\"\n",
    "        max_count = None\n",
    "        for pair, count in pair_count.items():\n",
    "            if max_count is None or max_count < count:\n",
    "                best_pair = pair\n",
    "                max_count = count\n",
    "        splits = merge_pair(*best_pair, splits, word_count)\n",
    "        merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "        book_chars = np.append(book_chars,best_pair[0] + best_pair[1])\n",
    "    return splits, merges, book_chars\n",
    "\n",
    "def tokenize(words, merges):\n",
    "    word_count = {}\n",
    "    for word in words:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "    \n",
    "    splits =  [[c for c in word] for word in word_count.keys()]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "for word in words:\n",
    "    if word in word_count:\n",
    "        word_count[word] += 1\n",
    "    else:\n",
    "        word_count[word] = 1\n",
    "splits, merges, book_chars = BPE(book_data, 200, word_count)\n",
    "#print(len(book_chars))\n",
    "tokenized_text = tokenize(words, merges)\n",
    "#rint(len(tokenized_text))\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "print(tokenized_text)\n",
    "print(char_to_ind)\n",
    "print(ind_to_char)\n",
    "book_data_ind = np.array([char_to_ind[c] for c in tokenized_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
