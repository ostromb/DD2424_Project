{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2424 Project in Deep Learning in Data Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "vol1_filename = '../Dataset/Training/edgar1.txt'\n",
    "vol2_filename = '../Dataset/Training/edgar2.txt'\n",
    "vol3_filename = '../Dataset/Training/edgar3.txt'\n",
    "vol4_filename = '../Dataset/Training/edgar4.txt'\n",
    "vol5_filename = '../Dataset/Training/edgar5.txt'\n",
    "\n",
    "encoding = \"utf8\"\n",
    "book_data_vol1 = np.array(load_data(vol1_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol2 = np.array(load_data(vol2_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol3 = np.array(load_data(vol3_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol4 = np.array(load_data(vol4_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol5 = np.array(load_data(vol5_filename, remove_footnotes=False, encoding=encoding))\n",
    "\n",
    "with open(vol1_filename,encoding=encoding,mode='r') as f:\n",
    "    vol1_txt = f.read()\n",
    "    words = vol1_txt.split()\n",
    "\n",
    "ngram_validation_text = \"\"\n",
    "for filename in [vol1_filename, vol2_filename, vol3_filename, vol4_filename, vol5_filename]:\n",
    "    with open(filename, encoding=encoding,mode='r') as file:\n",
    "        ngram_validation_text += file.read()\n",
    "\n",
    "with open('../Dataset/Training/synonyms.csv', encoding=encoding, mode=\"r\") as file:\n",
    "    all_synonyms = file.read()   \n",
    "\n",
    "all_book_data = np.concatenate((book_data_vol1, book_data_vol2, book_data_vol3, book_data_vol4, book_data_vol5))\n",
    "book_chars = np.unique(np.concatenate((all_book_data, np.array([s for s in all_synonyms]))))\n",
    "print(len(book_chars))\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "\n",
    "vocab_size = len(book_chars)\n",
    "embedding_dim = 256 \n",
    "\n",
    "\n",
    "def create_batches(data, batch_size, seq_length):\n",
    "    book_data_ind = np.array([char_to_ind[c] for c in data])\n",
    "\n",
    "    # Split data into sequences\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(book_data_ind)\n",
    "    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "    # Split data into X, Y\n",
    "    dataset = sequences.map(split_input_target)\n",
    "\n",
    "    # This organizes the data into groups of sequences. batch_size denotes the number of sequences in a batch, and seq_length denotes the number of characters in a sequence.\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(rnn_units, batch_size, nr_lstm_layers, bn):\n",
    "    m = tf.keras.Sequential()\n",
    "    m.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    for i in range(nr_lstm_layers):\n",
    "        m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    if bn:\n",
    "        m.add(tf.keras.layers.BatchNormalization())\n",
    "    m.add(tf.keras.layers.Dense(vocab_size))\n",
    "    return m\n",
    "\n",
    "\n",
    "def train_model(dataset_train, dataset_val, eta, rnn_units, n_epochs, batch_size, output_filename, nr_lstm_layers=1, bn=False):\n",
    "    m = build_model(rnn_units, batch_size, nr_lstm_layers, bn)\n",
    "    for i_ex, t_ex in dataset_train.take(1):\n",
    "        example_pred = m(i_ex)  # this step builds the model\n",
    "\n",
    "    # Specify update rule and compile model\n",
    "    adam_opt = tf.keras.optimizers.Adam(learning_rate=eta)\n",
    "    loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    m.compile(optimizer=adam_opt, loss=loss_func)\n",
    "\n",
    "    # Configure checkpoints\n",
    "    current_dir_path = os.getcwd()\n",
    "    checkpoint_dir = os.path.join(os.path.join(os.path.join(current_dir_path, \"tmp\"), output_filename), \"training_checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch:02d}.hdf5')\n",
    "    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True, save_best_onl =False)\n",
    "\n",
    "    # train\n",
    "    if dataset_val:\n",
    "        history = m.fit(x=dataset_train, epochs=n_epochs, validation_data=dataset_val, callbacks=[checkpoint_callback])\n",
    "    else:\n",
    "        history = m.fit(x=dataset_train, epochs=n_epochs, callbacks=[checkpoint_callback])\n",
    "    return m, history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "223/223 [==============================] - 563s 3s/step - loss: 3.0324 - val_loss: 2.9835\n",
      "Epoch 2/5\n",
      "157/223 [====================>.........] - ETA: 1:54 - loss: 2.9609"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of RNN units.\n",
    "rnn_units = 500\n",
    "n_epochs = 5\n",
    "eta = 0.01\n",
    "batch_size = 50\n",
    "seq_length = 50\n",
    "\n",
    "modelpathname = 'v4_2'\n",
    "nr_layers = 2\n",
    "\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "#augmented_data_str, used_synonyms = augment_data(validation_text, n_synonyms=1000, n_word_swaps=1000, n_deletions=500, n_sentence_swaps=500)\n",
    "dataset_train = create_batches(book_data_vol1, batch_size, seq_length)\n",
    "dataset_val = create_batches(book_data_vol2, batch_size, seq_length)\n",
    "\n",
    "use_bn = False\n",
    "model, history= train_model(dataset_train, dataset_val, eta, rnn_units, n_epochs, batch_size, modelpathname, nr_layers, use_bn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    if file != \".DS_Store\":\n",
    "        e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "        if e>latest_epoch:\n",
    "            latest_epoch = e\n",
    "            latest_checkpoint_file = file\n",
    "print(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "\n",
    "m = build_model(rnn_units=rnn_units, batch_size=1, nr_lstm_layers=nr_layers, bn=use_bn)\n",
    "m.load_weights(os.path.join(checkpoint_dir, latest_checkpoint_file))\n",
    "m.build(tf.TensorShape([1, None]))\n",
    "\n",
    "inputt = ngram_validation_text[1000:1200]\n",
    "validation_text = ngram_validation_text\n",
    "gen_text = generate_text(model=m, start_string=inputt, text_size=1000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=1, p=None)\n",
    "nmax = 2\n",
    "fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=nmax)\n",
    "repetition_score2 = measure_diversity(text_generated=gen_text, n_max=nmax)\n",
    "\n",
    "print(gen_text)\n",
    "print(fraction_correct_words, bleu_score2, repetition_score2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "modelpathname = \"1layer_gridsearch\"\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "output_str = \"batch_size | seq_len | eta | loss | BLEU2 \\n\"\n",
    "\n",
    "batch_sizes = [25, 75, 125]\n",
    "seq_lengths = [25, 75, 125]\n",
    "learningrates = [0.1, 0.01, 0.001]\n",
    "count = 0\n",
    "for batch_size in batch_sizes:\n",
    "    for seq_length in seq_lengths:\n",
    "        dataset = create_batches(book_data, batch_size, seq_length)\n",
    "        for eta in learningrates:\n",
    "            print(count/27)\n",
    "            model, history = train_model(dataset, eta, rnn_units, n_epochs, batch_size, modelpathname, 1, False)\n",
    "            final_loss = history.history['loss'][-1]\n",
    "            # generate text\n",
    "            m = build_model(rnn_units=rnn_units, batch_size=1, nr_lstm_layers=1, bn=False)\n",
    "            m.load_weights(checkpoint_dir + \"/\" + \"ckpt_0{}.hdf5\".format(n_epochs))\n",
    "            m.build(tf.TensorShape([1, None]))\n",
    "            gen_text = generate_text(model=m, start_string=\"The \", text_size=1000, \n",
    "                                    char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=1.0, p=None)\n",
    "            # Measure performance\n",
    "            frac_corr_words, bleu2 = measure_bleu(gen_text, validation_text, 2)\n",
    "            output_str += \"{}      {}      {}      {}      {} \\n\".format(batch_size, seq_length, eta, final_loss, bleu2)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "print(output_str)  \n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text, evauate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "plot_title = 'Loss over epochs'\n",
    "\n",
    "plotbool = True\n",
    "outputfile_bool = True\n",
    "\n",
    "# Find latest checkpoint file, because tf.train.latest_checkpoint(dir) doesn't work for some reason\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    if file != \".DS_Store\":\n",
    "        e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "        if e>latest_epoch:\n",
    "            latest_epoch = e\n",
    "            latest_checkpoint_file = file\n",
    "print(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "\n",
    "\n",
    "m = build_model(rnn_units=rnn_units, batch_size=1, nr_lstm_layers=1, bn=False)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([1, None]))\n",
    "\n",
    "nucleus_probability = None\n",
    "temp = 1.0\n",
    "gen_text = generate_text(model=m, start_string=\"The \", text_size=1000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=temp, p=nucleus_probability)\n",
    "\n",
    "\n",
    "if plotbool:\n",
    "    d = get_n_grams(gen_text, 1)\n",
    "    fig = px.line(history.history['loss'], title=plot_title, width=600)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_xaxes(title_text=\"Iteration step, in multiples of 10k\")\n",
    "    fig.update_yaxes(title_text=\"smoothed loss\")\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_data, n_max=2)\n",
    "repetition_score2 = measure_diversity(text_generated=gen_text, n_max=2)\n",
    "\n",
    "output_str = gen_text + \"\\n ------------ \\n loss {} \\n fraction of correctly spelled words: {} \\n Bleu score2: {}, Repetition score2: {} \".format(history.history['loss'], fraction_correct_words, bleu_score2, repetition_score2)\n",
    "output_str += \"\\n settings: batch_size, seq_length, eta, rnn_units, n_epochs = {}, {}, {}, {}, {}\".format(batch_size, seq_length, eta, rnn_units, n_epochs)\n",
    "print(output_str)\n",
    "\n",
    "\n",
    "# Create output file\n",
    "output_path = os.path.join(os.path.join(os.getcwd(), \"tmp\"), modelpathname)\n",
    "if outputfile_bool:\n",
    "    with open(os.path.join(output_path, \"perf.txt\"), \"w\") as file:\n",
    "        file.write(output_str)   \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"wait on,verb,serve;attend to;attend;assist\"\n",
    "line2 = \"unmake,verb,undo\"\n",
    "word = line.split(\",\")[0]\n",
    "synonyms = line.split(\",\")[2].split(\";\")\n",
    "print(synonyms)\n",
    "\n",
    "print(line2.split(\",\")[2].split(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filename = '../Dataset/Training/edgar1.txt'\n",
    "book_data = np.array(load_data(training_data_filename))\n",
    "book_chars = np.unique(book_data)\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "m = 100\n",
    "eta = 0.1\n",
    "seq_length = 25\n",
    "sig = 0.01\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m, k, eta, seq_length, sig)\n",
    "h0 = np.zeros((m, 1))\n",
    "max_iter = 200000\n",
    "epochs = 10\n",
    "smoothloss_list = []\n",
    "loss_list = []\n",
    "iteration = 0\n",
    "smoothloss = 0\n",
    "sentences = []\n",
    "for i in range(epochs):\n",
    "    rnn.hprev = np.zeros((m, 1))\n",
    "    for e in range(0, book_data.shape[0]-seq_length-1, seq_length):\n",
    "        X_chars = book_data[e:e+seq_length]\n",
    "        Y_chars = book_data[e+1:e+seq_length+1]\n",
    "        X = one_hot_encoding(X_chars, char_to_ind, k)\n",
    "        Y = one_hot_encoding(Y_chars, char_to_ind, k)\n",
    "        loss = rnn.adagrad(X, Y, h0, iteration)\n",
    "        if smoothloss == 0:\n",
    "            smoothloss = loss\n",
    "        smoothloss = 0.999*smoothloss + 0.001*loss\n",
    "     \n",
    "        if iteration % 10000 == 1:\n",
    "            print('Iteration: {}, Loss: {} '.format(iteration, smoothloss))\n",
    "            y = rnn.synthetize(rnn.hprev, X[:, 0], 200)\n",
    "            sentence = one_hot_decoding(y, ind_to_char)\n",
    "            print(sentence + \"\\n\")\n",
    "            #sentences.append(sentence)\n",
    "            smoothloss_list.append(smoothloss)\n",
    "            loss_list.append(loss)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration>max_iter:\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(training_data_filename,encoding='cp850',mode='r') as file:\n",
    "    validation_text = file.read()\n",
    "start_char = \"T\"\n",
    "start_char_onehot = one_hot_encoding(start_char, char_to_ind, k)\n",
    "generated_text_vanilla_onehot = rnn.synthetize(rnn.hprev, start_char_onehot, 1000)\n",
    "generated_text_vanilla = start_char + one_hot_decoding(generated_text_vanilla_onehot, ind_to_char)\n",
    "print(generated_text_vanilla)\n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "nmax = 4\n",
    "fraction_correct_words, bleu_score = measure_bleu(text_generated=generated_text_vanilla, text_val=validation_text, n_max=nmax)\n",
    "repetition_score = measure_diversity(text_generated=generated_text_vanilla, n_max=nmax)\n",
    "print(\"\\n loss function\", loss_list)\n",
    "print(\"\\n fraction of correctly spelled words: {} \\n Bleu score: {} \\n Repetition score: {}\".format(fraction_correct_words, bleu_score, repetition_score))\n",
    "\n",
    "\n",
    "\n",
    "fig = px.line(smoothloss_list, title='Smoothed loss over epochs', width=600)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_yaxes(title_text=\"smoothed loss\")\n",
    "fig.update_xaxes(title_text=\"iteration step, in multiples of 10k\")\n",
    "fig.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
