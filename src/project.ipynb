{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2424 Project in Deep Learning in Data Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filename = \"../Dataset/Training/vol1.txt\"\n",
    "book_data = np.array(load_data(training_data_filename))\n",
    "book_chars = np.unique(book_data)\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "m = 100\n",
    "eta = 0.1\n",
    "seq_length = 25\n",
    "sig = 0.01\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m, k, eta, seq_length, sig)\n",
    "h0 = np.zeros((m, 1))\n",
    "epochs = 10\n",
    "losslist = []\n",
    "iteration = 0\n",
    "smoothloss = 0\n",
    "sentences = []\n",
    "for i in range(epochs):\n",
    "    rnn.hprev = np.zeros((m, 1))\n",
    "    for e in range(0, book_data.shape[0]-seq_length-1, seq_length):\n",
    "        X_chars = book_data[e:e+seq_length]\n",
    "        Y_chars = book_data[e+1:e+seq_length+1]\n",
    "        X = one_hot_encoding(X_chars, char_to_ind, k)\n",
    "        Y = one_hot_encoding(Y_chars, char_to_ind, k)\n",
    "        loss = rnn.adagrad(X, Y, h0)\n",
    "        if smoothloss == 0:\n",
    "            smoothloss = loss\n",
    "        smoothloss = 0.999*smoothloss + 0.001*loss\n",
    "     \n",
    "        if iteration % 10000 == 0:\n",
    "            print('Iteration: {}, Loss: {}'.format(iteration, smoothloss))\n",
    "            y = rnn.synthetize(rnn.hprev, X[:, 0], 200)\n",
    "            sentence = one_hot_decoding(y, ind_to_char)\n",
    "            print(sentence)\n",
    "            sentences.append(sentence)\n",
    "            losslist.append(smoothloss)\n",
    "        iteration += 1\n",
    "        if iteration>400001:\n",
    "            break\n",
    "    if iteration>400001:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filename = \"../Dataset/Training/vol1.txt\"\n",
    "book_data = np.array(load_data(training_data_filename,remove_footnotes=False))\n",
    "book_chars = np.unique(book_data)\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "batch_size = 50\n",
    "seq_length = 50\n",
    "\n",
    "book_data_ind = np.array([char_to_ind[c] for c in book_data])\n",
    "# Split data into sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(book_data_ind)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(data):\n",
    "    input = data[:-1]\n",
    "    target = data[1:]\n",
    "    return input, target\n",
    "\n",
    "# Split data into X, Y\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# This organizes the data into groups of sequences. batch_size denotes the number of sequences in a batch, and seq_length denotes the number of characters in a sequence.\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " 'e folio\\nMS., and are printed i'\n",
      "Next char prediction:\n",
      " 'b-A&4%)┤╣é$kfp╗ñ{8Jqb¼@5FWN0u®'\n",
      "813/813 [==============================] - 19s 19ms/step - loss: 2.2696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def loss(Y_true,Y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true=Y_true, y_pred=Y_pred, from_logits=True)\n",
    "\n",
    "# Length of the vocabulary in chars.\n",
    "vocab_size = len(book_chars)\n",
    "\n",
    "# The embedding dimension.\n",
    "embedding_dim = 200\n",
    "\n",
    "# Number of RNN units.\n",
    "rnn_units = 200\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    m = tf.keras.Sequential()\n",
    "    m.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    m.add(tf.keras.layers.BatchNormalization(synchronized=True))\n",
    "    m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    m.add(tf.keras.layers.Dense(vocab_size))\n",
    "    #m.summary()\n",
    "    return m\n",
    "\n",
    "m = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "\n",
    "# try the model\n",
    "for i_ex, t_ex in dataset.take(1):\n",
    "    example_pred = m(i_ex)  # this step builds the model\n",
    "    sampled_indices = tf.random.categorical(logits=example_pred[0], num_samples=1)\n",
    "    sample_ind_1d = tf.squeeze(input=sampled_indices, axis=-1).numpy()\n",
    "print('Input:\\n', repr(''.join([ind_to_char[c] for c in i_ex.numpy()[0]])))\n",
    "print('Next char prediction:\\n', repr(''.join([ind_to_char[c] for c in sample_ind_1d])))  \n",
    "\n",
    "# Directory where the checkpoints will be saved.\n",
    "current_dir_path = os.getcwd()\n",
    "checkpoint_dir = current_dir_path + \"\\\\tmp\\\\checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch:02d}.hdf5')\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True, \n",
    "    save_best_onl =True\n",
    ")\n",
    "\n",
    "# Specify update rule and compile model\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "m.compile(optimizer=adam_opt, loss=loss)\n",
    "\n",
    "# train\n",
    "history = m.fit(x=dataset, epochs=1, callbacks=[checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the coping hid, thougrasties'.          Ire rositz.\n",
      "\n",
      "\n",
      "28-175) the vitzs mer. (Ol\n",
      "[Foto nure blol ingut hall cher Vouly coep_ts and mave's for Lockssecke; wat shese',\n",
      "  Nor Histy\n",
      "boll Magoterbigle, \"Our,) fagldericallerg the Eurceran mesis najecers. Is foultry of\n",
      "commice allselsat, has slistione by than by alsron, in alprent of I can bain's, to Spee ofk\n",
      "  you\n",
      "rispleed,-1.\n",
      "  Bide's and be; hally, chil of 1803.\n",
      "  The mare might to G. Coperish Shawhemt wa . _N's hight, ergestet;\n",
      "  T no uspry in, and oby\n",
      "('Nettions, auther a shalll this said, yation;\n",
      "  But work as fince at, who sastear meng bleoight; wapleents sexprew's projeccanbert, lline his sirnsd distic. Leagen Foore\n",
      "Reatuound abty rearating--nocceop;\" 1817. The loved of\n",
      "  To the\n",
      "Porged tomards'\n",
      "\n",
      "\"Ocation; poosk:\n",
      "  enther weored Hy this this 'watorn tronitionic\n",
      "  Veentle his all eghe,\n",
      "ignions.\n",
      "  'MA.\n",
      "['.' The a heu ut\n",
      "\n",
      "\n",
      "1, Theak as;\n",
      "  Barks, whulers, vert--none, lating flool,' fay, the \"blequenhered, ingresple--\n",
      "\n",
      "['MS. [F.S' outsumele\n"
     ]
    }
   ],
   "source": [
    "# Find latest checkpoint file, because tf.train.latest_checkpoint(dir) doesn't work for some reason\n",
    "# latest_epoch = 0\n",
    "# latest_checkpoint_file = \"\"\n",
    "# for file in os.listdir(checkpoint_dir):\n",
    "#     e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "#     if e>latest_epoch:\n",
    "#         latest_epoch = e\n",
    "#         latest_checkpoint_file = file\n",
    "# print(latest_checkpoint_file)\n",
    "latest_checkpoint_file = \"ckpt_01.hdf5\"\n",
    "\n",
    "simplified_batch_size = 1\n",
    "m = build_model(vocab_size, embedding_dim, rnn_units, simplified_batch_size)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([simplified_batch_size, None]))\n",
    "\n",
    "# Generate text\n",
    "gen_text = generate_text(model=m, start_string=u\"the\", text_size=1000, char_to_ind=char_to_ind, ind_to_char=ind_to_char)\n",
    "print(gen_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of correctly spelled words: 0.4409937888198758 \n",
      " Bleu score: 0.00026698068015285157 \n",
      " Repetition score: 0.0012924051150995462\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance metrics for generated text\n",
    "with open(training_data_filename) as file:\n",
    "    validation_text = file.read()\n",
    "\n",
    "fraction_correct_words, bleu_score = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=2)\n",
    "repetition_score = measure_diversity(text_generated=gen_text, n_max=2)\n",
    "print(\"fraction of correctly spelled words: {} \\n Bleu score: {} \\n Repetition score: {}\".format(fraction_correct_words, bleu_score, repetition_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
