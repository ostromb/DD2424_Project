{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2424 Project in Deep Learning in Data Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filename = '../Dataset/Training/edgar1.txt'\n",
    "book_data = np.array(load_data(training_data_filename))\n",
    "book_chars = np.unique(book_data)\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "m = 100\n",
    "eta = 0.1\n",
    "seq_length = 25\n",
    "sig = 0.01\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m, k, eta, seq_length, sig)\n",
    "h0 = np.zeros((m, 1))\n",
    "max_iter = 200000\n",
    "epochs = 10\n",
    "smoothloss_list = []\n",
    "loss_list = []\n",
    "iteration = 0\n",
    "smoothloss = 0\n",
    "sentences = []\n",
    "for i in range(epochs):\n",
    "    rnn.hprev = np.zeros((m, 1))\n",
    "    for e in range(0, book_data.shape[0]-seq_length-1, seq_length):\n",
    "        X_chars = book_data[e:e+seq_length]\n",
    "        Y_chars = book_data[e+1:e+seq_length+1]\n",
    "        X = one_hot_encoding(X_chars, char_to_ind, k)\n",
    "        Y = one_hot_encoding(Y_chars, char_to_ind, k)\n",
    "        loss = rnn.adagrad(X, Y, h0, iteration)\n",
    "        if smoothloss == 0:\n",
    "            smoothloss = loss\n",
    "        smoothloss = 0.999*smoothloss + 0.001*loss\n",
    "     \n",
    "        if iteration % 10000 == 1:\n",
    "            print('Iteration: {}, Loss: {} '.format(iteration, smoothloss))\n",
    "            y = rnn.synthetize(rnn.hprev, X[:, 0], 200)\n",
    "            sentence = one_hot_decoding(y, ind_to_char)\n",
    "            print(sentence + \"\\n\")\n",
    "            #sentences.append(sentence)\n",
    "            smoothloss_list.append(smoothloss)\n",
    "            loss_list.append(loss)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration>max_iter:\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(training_data_filename,encoding='cp850',mode='r') as file:\n",
    "    validation_text = file.read()\n",
    "start_char = \"T\"\n",
    "start_char_onehot = one_hot_encoding(start_char, char_to_ind, k)\n",
    "generated_text_vanilla_onehot = rnn.synthetize(rnn.hprev, start_char_onehot, 1000)\n",
    "generated_text_vanilla = start_char + one_hot_decoding(generated_text_vanilla_onehot, ind_to_char)\n",
    "print(generated_text_vanilla)\n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "nmax = 4\n",
    "fraction_correct_words, bleu_score = measure_bleu(text_generated=generated_text_vanilla, text_val=validation_text, n_max=nmax)\n",
    "repetition_score = measure_diversity(text_generated=generated_text_vanilla, n_max=nmax)\n",
    "print(\"\\n loss function\", loss_list)\n",
    "print(\"\\n fraction of correctly spelled words: {} \\n Bleu score: {} \\n Repetition score: {}\".format(fraction_correct_words, bleu_score, repetition_score))\n",
    "\n",
    "\n",
    "\n",
    "fig = px.line(smoothloss_list, title='Smoothed loss over epochs', width=600)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_yaxes(title_text=\"smoothed loss\")\n",
    "fig.update_xaxes(title_text=\"iteration step, in multiples of 10k\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "vol1_filename = '../Dataset/Training/edgar1.txt'\n",
    "vol2_filename = '../Dataset/Training/edgar2.txt'\n",
    "vol3_filename = '../Dataset/Training/edgar3.txt'\n",
    "vol4_filename = '../Dataset/Training/edgar4.txt'\n",
    "vol5_filename = '../Dataset/Training/edgar5.txt'\n",
    "\n",
    "encoding = \"utf8\"\n",
    "book_data_vol1 = np.array(load_data(vol1_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol2 = np.array(load_data(vol2_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol3 = np.array(load_data(vol3_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol4 = np.array(load_data(vol4_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol5 = np.array(load_data(vol5_filename, remove_footnotes=False, encoding=encoding))\n",
    "\n",
    "with open(vol1_filename,encoding=encoding,mode='r') as f:\n",
    "   words = f.read().split()\n",
    "with open(vol2_filename ,encoding=encoding,mode='r') as file:\n",
    "    validation2_text = file.read()\n",
    "    # takes a little time due to synonym swap\n",
    "with open('../Dataset/Training/synonyms.csv', encoding=encoding, mode=\"r\") as file:\n",
    "    all_synonyms = file.read()   \n",
    "\n",
    "# book_chars = np.unique(book_data)\n",
    "all_book_data = np.concatenate((book_data_vol1, book_data_vol2, book_data_vol3, book_data_vol4))\n",
    "book_chars = np.unique(np.concatenate((all_book_data, np.array([s for s in all_synonyms]))))\n",
    "print(len(book_chars))\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "\n",
    "vocab_size = len(book_chars)\n",
    "embedding_dim = 256 \n",
    "\n",
    "\n",
    "def create_batches(data, batch_size, seq_length):\n",
    "    book_data_ind = np.array([char_to_ind[c] for c in data])\n",
    "\n",
    "    # Split data into sequences\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(book_data_ind)\n",
    "    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "    # Split data into X, Y\n",
    "    dataset = sequences.map(split_input_target)\n",
    "\n",
    "    # This organizes the data into groups of sequences. batch_size denotes the number of sequences in a batch, and seq_length denotes the number of characters in a sequence.\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(rnn_units, batch_size, nr_lstm_layers, bn):\n",
    "    m = tf.keras.Sequential()\n",
    "    m.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    for i in range(nr_lstm_layers):\n",
    "        m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    if bn:\n",
    "        m.add(tf.keras.layers.BatchNormalization())\n",
    "    m.add(tf.keras.layers.Dense(vocab_size))\n",
    "    return m\n",
    "\n",
    "\n",
    "def train_model(dataset_train, dataset_val, eta, rnn_units, n_epochs, batch_size, output_filename, nr_lstm_layers=1, bn=False):\n",
    "    m = build_model(rnn_units, batch_size, nr_lstm_layers, bn)\n",
    "    for i_ex, t_ex in dataset_train.take(1):\n",
    "        example_pred = m(i_ex)  # this step builds the model\n",
    "\n",
    "    # Specify update rule and compile model\n",
    "    adam_opt = tf.keras.optimizers.Adam(learning_rate=eta)\n",
    "    loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    m.compile(optimizer=adam_opt, loss=loss_func)\n",
    "\n",
    "    # Configure checkpoints\n",
    "    current_dir_path = os.getcwd()\n",
    "    checkpoint_dir = os.path.join(os.path.join(os.path.join(current_dir_path, \"tmp\"), output_filename), \"training_checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch:02d}.hdf5')\n",
    "    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True, save_best_onl =False)\n",
    "\n",
    "    # train\n",
    "    history = m.fit(x=dataset_train, epochs=n_epochs, validation_data=dataset_val, callbacks=[checkpoint_callback])\n",
    "    return m, history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "modelpathname = \"1layer_gridsearch\"\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "output_str = \"batch_size | seq_len | eta | loss | BLEU2 \\n\"\n",
    "\n",
    "batch_sizes = [25, 75, 125]\n",
    "seq_lengths = [25, 75, 125]\n",
    "learningrates = [0.1, 0.01, 0.001]\n",
    "count = 0\n",
    "for batch_size in batch_sizes:\n",
    "    for seq_length in seq_lengths:\n",
    "        dataset = create_batches(book_data, batch_size, seq_length)\n",
    "        for eta in learningrates:\n",
    "            print(count/27)\n",
    "            model, history = train_model(dataset, eta, rnn_units, n_epochs, batch_size, modelpathname, 1, False)\n",
    "            final_loss = history.history['loss'][-1]\n",
    "            # generate text\n",
    "            m = build_model(rnn_units=rnn_units, batch_size=1, nr_lstm_layers=1, bn=False)\n",
    "            m.load_weights(checkpoint_dir + \"/\" + \"ckpt_0{}.hdf5\".format(n_epochs))\n",
    "            m.build(tf.TensorShape([1, None]))\n",
    "            gen_text = generate_text(model=m, start_string=\"The \", text_size=1000, \n",
    "                                    char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=1.0, p=None)\n",
    "            # Measure performance\n",
    "            frac_corr_words, bleu2 = measure_bleu(gen_text, validation_text, 2)\n",
    "            output_str += \"{}      {}      {}      {}      {} \\n\".format(batch_size, seq_length, eta, final_loss, bleu2)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "print(output_str)  \n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 29/223 [==>...........................] - ETA: 29s - loss: 3.0055"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of RNN units.\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "eta = 0.01\n",
    "batch_size = 50\n",
    "seq_length = 50\n",
    "\n",
    "modelpathname = 'v7_1'\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "#augmented_data_str, used_synonyms = augment_data(validation_text, n_synonyms=1000, n_word_swaps=1000, n_deletions=500, n_sentence_swaps=500)\n",
    "dataset_train = create_batches(book_data_vol1, batch_size, seq_length)\n",
    "dataset_val = create_batches(book_data_vol2, batch_size, seq_length)\n",
    "\n",
    "nr_layers = 1\n",
    "model, history= train_model(dataset_train, dataset_val, eta, rnn_units, n_epochs, batch_size, modelpathname, nr_layers, False)\n",
    "\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    if file != \".DS_Store\":\n",
    "        e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "        if e>latest_epoch:\n",
    "            latest_epoch = e\n",
    "            latest_checkpoint_file = file\n",
    "print(checkpoint_dir + \"/\" + latest_checkpoint_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefect, “from the nature\n",
      "      of the document, and from the non-appearance of certain results\n",
      "      which would at once arise from its passing out of the robber’s\n",
      "      possession; that is to say, from his employing it as he must\n",
      "      design in the end to employ it.”\n",
      "\n",
      "      “Be a little more explicit,” I said.\n",
      "\n",
      "      “Well, I may venture so far as to say that the paper gives its\n",
      "      holder a certain power in a certain quarter where such power is\n",
      "      immensely valuable.” The Prefect was fo\n",
      "l. The\n",
      "      atming\n",
      "      and gensension thus\n",
      "trowed utsefted at dua the old situately moon of Marius.\n",
      "      “Wh his graved\n",
      "      the attemitives of evidence\n",
      "      will brite dwerdong within the improver in ming, This shrever appenders as I bew you intermumbers of\n",
      "      Legrand to dettently, distance, I seemiled it; them. It and diy ears. He nortrawed flaming up. Forness\n",
      "     genience of and.\n",
      "\n",
      "      “I been\n",
      "      perfection, when\n",
      "       one seem the do occase of the fact to a the subject, this to\n",
      "      belown, of the sugggue,\n",
      "      Colliaphief could has ranget was it moment long hopen michor and \n",
      "0.6050955414012739 0.27852531565676714\n"
     ]
    }
   ],
   "source": [
    "data_val_txt = validation2_text\n",
    "\n",
    "\n",
    "m = build_model(rnn_units=rnn_units, batch_size=1, nr_lstm_layers=1, bn=False)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([1, None]))\n",
    "\n",
    "input_text = validation2_text[5000:5500]\n",
    "print(input_text)\n",
    "validation_data = validation2_text[6000:]\n",
    "gen_text = generate_text(model=m, start_string=input_text, text_size=1000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=1, p=None)\n",
    "fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_data, n_max=2)\n",
    "repetition_score2 = measure_diversity(text_generated=gen_text, n_max=2)\n",
    "\n",
    "print(gen_text)\n",
    "print(fraction_correct_words, bleu_score2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "223/223 [==============================] - 33s 136ms/step - loss: 2.1319\n",
      "Epoch 2/5\n",
      "223/223 [==============================] - 28s 124ms/step - loss: 1.6811\n",
      "Epoch 3/5\n",
      "223/223 [==============================] - 26s 114ms/step - loss: 1.5756\n",
      "Epoch 4/5\n",
      "223/223 [==============================] - 26s 115ms/step - loss: 1.5176\n",
      "Epoch 5/5\n",
      "223/223 [==============================] - 26s 116ms/step - loss: 1.4827\n",
      "c:\\Users\\sofia\\Documents\\skola\\Djupinlärning\\DD2424_Project\\src\\tmp\\augmented/v1\\training_checkpoints/ckpt_05.hdf5\n",
      "0\n",
      "1\n",
      "   \n",
      "['Prefect', ' “from the nature\\n      of the document', ' and from the non-appearance of certain results\\n      which would at once arise from its passing out of the robber’s\\n      possession; that is to say', ' from his employing it as he must\\n      design in the end to employ it', '”\\n\\n      “Be a little more explicit', '” I said', '\\n\\n      “Well', ' I may venture so far as to say that the paper gives its\\n      holder a certain power in a certain quarter where such power is\\n      immensely valuable', '” The Prefect was fond of the cant of\\n      diplomacy', '\\n\\n      “Still I do not quite understand', '” said Dupin', '\\n\\n      “No? Well; the disclosure of the document to a third person', ' who\\n      shall be nameless', ' would bring in question the honor of a\\n      personage of most exalted station; and this fact gives the holder\\n      of the document an ascendancy over the illustrious personage\\n      whose honor and peace are so jeopardized', '”\\n\\n      “But this ascendancy', '” I interposed', ' “would depend upon the\\n      robber’s knowledge of the loser’s knowledge of the robber', ' Who\\n      would dare—”\\n\\n      “The thief', '” said G', ' “is the Minister D——', ' who dares all things', '\\n      those unbecoming as well as those becoming a man', ' The method of\\n      the theft was not less ingenious than bold', ' The document in\\n      question—a letter', ' to be frank—had been received by the personage\\n      robbed while alone in the royal boudoir', ' During its perusal she\\n      was suddenly interrupted by the entrance of the other exalted\\n      personage from whom especially it was her wish to conceal it', '\\n      After a hurried and vain endeavor to thrust it in a drawer', ' she\\n      was forced to place it', ' open as it was', ' upon a table', ' The\\n      address', ' however', ' was uppermost', ' and', ' the contents thus\\n      unexposed', ' the letter escaped notice', ' At this juncture enters the\\n      Minister D——', ' His lynx eye immediately perceives the paper', '\\n      recognises the handwriting of the address', ' observes the confusion\\n      of the personage addressed', ' and fathoms her secret', ' After some\\n      business transactions', ' hurried through in his ordinary manner', ' he\\n      produces a letter somewhat similar to the one in question', ' opens\\n      it', ' pretends to read it', ' and then places it in close\\n      juxtaposition to the other', ' Again he converses', ' for some fifteen\\n      minutes', ' upon the public affairs', ' At length', ' in taking leave', ' he\\n      takes also from the table the letter to which he had no claim', '\\n      Its rightful owner saw', ' but', ' of course', ' dared not call attention\\n      to the act', ' in the presence of the third personage who stood at\\n      her elbow', ' The minister decamped; leaving his own letter—one of\\n      no importance—upon the table', '”\\n\\n      “Here', ' then', '” said Dupin to me', ' “you have precisely what you\\n      demand to make the ascendancy complete—the robber’s knowledge of\\n      the loser’s knowledge of the robber', '”\\n\\n      “Yes', '” replied the Prefect; “and the power thus attained has', ' for\\n      some months past', ' been wielded', ' for political purposes', ' to a very\\n      dangerous extent', ' The personage robbed is more thoroughly\\n      convinced', ' every day', ' of the necessity of reclaiming her letter', '\\n      But this', ' of course', ' cannot be done openly', ' In fine', ' driven to\\n      despair', ' she has committed the matter to me', '”\\n\\n      “Than whom', '” said Dupin', ' amid a perfect whirlwind of smoke', ' “no\\n      more sagacious agent could', ' I suppose', ' be desired', ' or even\\n      imagined', '”\\n\\n      “You flatter me', '” replied the Prefect; “but it is possible that\\n      some such opinion may have been entertained', '”\\n\\n      “It is clear', '” said I', ' “as you observe', ' that the letter is still\\n      in possession of the minister; since it is this possession', ' and\\n      not any employment of the letter', ' which bestows the power', ' With\\n      the employment the power departs', '”\\n\\n      “True', '” said G', '; “and upon this conviction I proceeded', ' My first\\n      care was to make thorough search of the minister’s hotel; and\\n      here my chief embarrassment lay in the necessity of searching\\n      without his knowledge', ' Beyond all things', ' I have been warned of\\n      the danger which would result from giving him reason to suspect\\n      our design', '”\\n\\n      “But', '” said I', ' “you are quite au fait in these investigations', '\\n      The Parisian police have done this thing often before', '”\\n\\n      “Oh', ' yes; and for this reason I did not despair', ' The habits of\\n      the minister gave me', ' too', ' a great advantage', ' He is frequently\\n      absent from home all night', ' His servants are by no means\\n      numerous', ' They sleep at a distance from their master’s apartment', '\\n      and', ' being chiefly Neapolitans', ' are readily made drunk', ' I have\\n      keys', ' as you know', ' with which I can open any chamber or cabinet\\n      in Paris', ' For three months a night has not passed', ' during the\\n      greater part of which I have not been engaged', ' personally', ' in\\n      ransacking the D—— Hotel', ' My honor is interested', ' and', ' to mention\\n      a great secret', ' the reward is enormous', ' So I did not abandon the\\n      search until I had become fully satisfied that the thief is a\\n      more astute man than myself', ' I fancy that I have investigated\\n      every nook and corner of the premises in which it is possible\\n      that the paper can be concealed', '”\\n\\n      “But is it not possible', '” I suggested', ' “that although the letter\\n      may be in possession of the minister', ' as it unquestionably is', ' he\\n      may have concealed it elsewhere than upon his own premises?”\\n\\n      “This is barely possible', '” said Dupin', ' “The present peculiar\\n      condition of affairs at court', ' and especially of those intrigues\\n      in which D—— is known to be involved', ' would render the instant\\n      availability of the document—its susceptibility of being produced\\n      at a moment’s notice—a point of nearly equal importance with its\\n      possession', '”\\n\\n      “Its susceptibility of being produced?” said I', '\\n\\n      “That is to say', ' of being destroyed', '” said Dupin', '\\n\\n      “True', '” I observed; “the paper is clearly then upon the premises', '\\n      As for its being upon the person of the minister', ' we may consider\\n      that as out of the question', '”\\n\\n      “Entirely', '” said the Prefect', ' “He has been twice waylaid', ' as if\\n      by footpads', ' and his person rigorously searched under my own\\n      inspection', '”\\n\\n      “You might have spared yourself this trouble', '” said Dupin', ' “D——', '\\n      I presume', ' is not altogether a fool', ' and', ' if not', ' must have\\n      anticipated these waylayings', ' as a matter of course', '”\\n\\n      “Not altogether a fool', '” said G', ' “but then he’s a poet', ' which I\\n      take to be only one remove from a fool', '”\\n\\n      “True', '” said Dupin', ' after a long and thoughtful whiff from his\\n      meerschaum', ' “although I have been guilty of certain doggrel\\n      myself', '”\\n\\n      “Suppose you detail', '” said I', ' “the particulars of your search', '”\\n\\n      “Why the fact is', ' we took our time', ' and we searched _everywhere_', '\\n      I have had long experience in these affairs', ' I took the entire\\n      building', ' room by room; devoting the nights of a whole week to\\n      each', ' We examined', ' first', ' the furniture of each apartment', ' We\\n      opened every possible drawer; and I presume you know that', ' to a\\n      properly trained police agent', ' such a thing as a secret drawer is\\n      impossible', ' Any man is a dolt who permits a ‘secret’ drawer to\\n      escape him in a search of this kind', ' The thing is so plain', ' There\\n      is a certain amount of bulk—of space—to be accounted for in every\\n      cabinet', ' Then we have accurate rules', ' The fiftieth part of a line\\n      could not escape us', ' After the cabinets we took the chairs', ' The\\n      cushions we probed with the fine long needles you have seen me\\n      employ', ' From the tables we removed the tops', '”\\n\\n      “Why so?”\\n\\n      “Sometimes the top of a table', ' or other similarly arranged piece\\n      of furniture', ' is removed by the person wishing to conceal an\\n      article; then the leg is excavated', ' the article deposited within\\n      the cavity', ' and the top replaced', ' The bottoms and tops of\\n      bedposts are employed in the same way', '”\\n\\n      “But could not the cavity be detected by sounding?” I asked', '\\n\\n      “By no means', ' if', ' when the article is deposited', ' a sufficient\\n      wadding of cotton be placed around it', ' Besides', ' in our case', ' we\\n      were obliged to proceed without noise', '”\\n\\n      “But you could not have removed—you could not have taken to\\n      pieces all articles of furniture in which it would have been\\n      possible to make a deposit in the manner you mention', ' A letter\\n      may be compressed into a thin spiral roll', ' not differing much in\\n      shape or bulk from a large knitting-needle', ' and in this form it\\n      might be inserted into the rung of a chair', ' for example', ' You did\\n      not take to pieces all the chairs?”\\n\\n      “Certainly not; but we did better—we examined the rungs of every\\n      chair in the hotel', ' and', ' indeed the jointings of every\\n      description of furniture', ' by the aid of a most powerful\\n      microscope', ' Had there been any traces of recent disturbance we\\n      should not have failed to detect it instantly', ' A single grain of\\n      gimlet-dust', ' for example', ' would have been as obvious as an apple', '\\n      Any disorder in the glueing—any unusual gaping in the\\n      joints—would have sufficed to insure detection', '”\\n\\n      “I presume you looked to the mirrors', ' between the boards and the\\n      plates', ' and you probed the beds and the bed-clothes', ' as well as\\n      the curtains and carpets', '”\\n\\n      “That of course; and when we had absolutely completed every\\n      particle of the furniture in this way', ' then we examined the house\\n      itself', ' We divided its entire surface into compartments', ' which we\\n      numbered', ' so that none might be missed; then we scrutinized each\\n      individual square inch throughout the premises', ' including the two\\n      houses immediately adjoining', ' with the microscope', ' as before', '”\\n\\n      “The two houses adjoining!” I exclaimed; “you must have had a\\n      great deal of trouble', '”\\n\\n      “We had; but the reward offered is prodigious!”\\n\\n      “You include the grounds about the houses?”\\n\\n      “All the grounds are paved with brick', ' They gave us comparatively\\n      little trouble', ' We examined the moss between the bricks', ' and\\n      found it undisturbed', '”\\n\\n      “You looked among D——‘s papers', ' of course', ' and into the books of\\n      the library?”\\n\\n      “Certainly; we opened every package and parcel; we not only\\n      opened every book', ' but we turned over every leaf in each volume', '\\n      not contenting ourselves with a mere shake', ' according to the\\n      fashion of some of our police officers', ' We also measured the\\n      thickness of every book-cover', ' with the most accurate\\n      admeasurement', ' and applied to each the most jealous scrutiny of\\n      the microscope', ' Had any of the bindings been recently meddled\\n      with', ' it would have been utterly impossible that the fact should\\n      have escaped observation', ' Some five or six volumes', ' just from the\\n      hands of the binder', ' we carefully probed', ' longitudinally', ' with\\n      the needles', '”\\n\\n      “You explored the floors beneath the carpets?”\\n\\n      “Beyond doubt', ' We removed every carpet', ' and examined the boards\\n      with the microscope', '”\\n\\n      “And the paper on the walls?”\\n\\n      “Yes', '”\\n\\n      “You looked into the cellars?”\\n\\n      “We did', '”\\n\\n      “Then', '” I said', ' “you have been making a miscalculation', ' and the\\n      letter is not upon the premises', ' as you suppose', '”\\n\\n      “I fear you are right there', '” said the Prefect', ' “And now', ' Dupin', '\\n      what would you advise me to do?”\\n\\n      “To make a thorough re-search of the premises', '”\\n\\n      “That is absolutely needless', '” replied G——', ' “I am not more sure\\n      that I breathe than I am that the letter is not at the Hotel', '”\\n\\n      “I have no better advice to give you', '” said Dupin', ' “You have', ' of\\n      course', ' an accurate description of the letter?”\\n\\n      “Oh yes!”—And here the Prefect', ' producing a memorandum-book', '\\n      proceeded to read aloud a minute account of the internal', ' and\\n      especially of the external appearance of the missing document', '\\n      Soon after finishing the perusal of this description', ' he took his\\n      departure', ' more entirely depressed in spirits than I had ever\\n      known the good gentleman before', '\\n\\n      In about a month afterwards he paid us another visit', ' and found\\n      us occupied very nearly as before', ' He took a pipe and a chair and\\n      entered into some ordinary conversation', ' At length I said', '—\\n\\n      “Well', ' but G——', ' what of the purloined letter? I presume you have\\n      at last made up your mind that there is no such thing as\\n      overreaching the Minister?”\\n\\n      “Confound him', ' say I—yes; I made the re-examination', ' however', ' as\\n      Dupin suggested—but it was all labor lost', ' as I knew it would\\n      be', '”\\n\\n      “How much was the reward offered', ' did you say?” asked Dupin', '\\n\\n      “Why', ' a very great deal—a very liberal reward—I don’t like to say\\n      how much', ' precisely; but one thing I will say', ' that I wouldn’t\\n      mind giving my individual check for fifty thousand francs to any\\n      one who could obtain me that letter', ' The fact is', ' it is becoming\\n      of more and more importance every day; and the reward has been\\n      lately doubled', ' If it were trebled', ' however', ' I could do no more\\n      than I have done', '”\\n\\n      “Why', ' yes', '” said Dupin', ' drawlingly', ' between the whiffs of his\\n      meerschaum', ' “I really—think', ' G——', ' you have not exerted\\n      yourself—to the utmost in this matter', ' You might—do a little\\n      more', ' I think', ' eh?”\\n\\n      “How?—in what way?”\\n\\n      “Why—puff', ' puff—you might—puff', ' puff—employ counsel in the\\n      matter', ' eh?—puff', ' puff', ' puff', ' Do you remember the story they tell\\n      of Abernethy?”\\n\\n      “No; hang Abernethy!”\\n\\n      “To be sure! hang him and welcome', ' But', ' once upon a time', ' a\\n      certain rich miser conceived the design of spunging upon this\\n      Abernethy for a medical opinion', ' Getting up', ' for this purpose', ' an\\n      ordinary conversation in a private company', ' he insinuated his\\n      case to the physician', ' as that of an imaginary individual', '\\n\\n      “‘We will suppose', '’ said the miser', ' ‘that his symptoms are such\\n      and such; now', ' doctor', ' what would you have directed him to take?’\\n\\n      “‘Take!’ said Abernethy', ' ‘why', ' take advice', ' to be sure', '’”\\n\\n      “But', '” said the Prefect', ' a little discomposed', ' “I am perfectly\\n      willing to take advice', ' and to pay for it', ' I would really give\\n      fifty thousand francs to any one who would aid me in the matter', '”\\n\\n      “In that case', '” replied Dupin', ' opening a drawer', ' and producing a\\n      check-book', ' “you may as well fill me up a check for the amount\\n      mentioned', ' When you have signed it', ' I will hand you the letter', '”\\n\\n      I was astounded', ' The Prefect appeared absolutely\\n      thunder-stricken', ' For some minutes he remained speechless and\\n      motionless', ' looking incredulously at my friend with open mouth', '\\n      and eyes that seemed starting from their sockets; then', '\\n      apparently recovering himself in some measure', ' he seized a pen', '\\n      and after several pauses and vacant stares', ' finally filled up and\\n      signed a check for fifty thousand francs', ' and handed it across\\n      the table to Dupin', ' The latter examined it carefully and\\n      deposited it in his pocket-book; then', ' unlocking an escritoire', '\\n      took thence a letter and gave it to the Prefect', ' This functionary\\n      grasped it in a perfect agony of joy', ' opened it with a trembling\\n      hand', ' cast a rapid glance at its contents', ' and then', ' scrambling\\n      and struggling to the door', ' rushed at length unceremoniously from\\n      the room and from the house', ' without having uttered a syllable\\n      since Dupin had requested him to fill up the check', '\\n\\n      When he had gone', ' my friend entered into some explanations', '\\n\\n      “The Parisian police', '” he said', ' “are exceedingly able in their\\n      way', ' They are persevering', ' ingenious', ' cunning', ' and thoroughly\\n      versed in the knowledge which their duties seem chiefly to\\n      demand', ' Thus', ' when G—— detailed to us his mode of searching the\\n      premises at the Hotel D——', ' I felt entire confidence in his having\\n      made a satisfactory investigation—so far as his labors extended', '”\\n\\n      “So far as his labors extended?” said I', '\\n\\n      “Yes', '” said Dupin', ' “The measures adopted were not only the best\\n      of their kind', ' but carried out to absolute perfection', ' Had the\\n      letter been deposited within the range of their search', ' these\\n      fellows would', ' beyond a question', ' have found it', '”\\n\\n      I merely laughed—but he seemed quite serious in all that he said', '\\n\\n      “The measures', ' then', '” he continued', ' “were good in their kind', ' and\\n      well executed; their defect lay in their being inapplicable to\\n      the case', ' and to the man', ' A certain set of highly ingenious\\n      resources are', ' with the Prefect', ' a sort of Procrustean bed', ' to\\n      which he forcibly adapts his designs', ' But he perpetually errs by\\n      being too deep or too shallow for the matter in hand; and many a\\n      schoolboy is a better reasoner than he', ' I knew one about eight\\n      years of age', ' whose success at guessing in the game of ‘even and\\n      odd’ attracted universal admiration', ' This game is simple', ' and is\\n      played with marbles', ' One player holds in his hand a number of\\n      these toys', ' and demands of another whether that number is even or\\n      odd', ' If the guess is right', ' the guesser wins one; if wrong', ' he\\n      loses one', ' The boy to whom I allude won all the marbles of the\\n      school', ' Of course he had some principle of guessing; and this lay\\n      in mere observation and admeasurement of the astuteness of his\\n      opponents', ' For example', ' an arrant simpleton is his opponent', ' and', '\\n      holding up his closed hand', ' asks', ' ‘are they even or odd?’ Our\\n      schoolboy replies', ' ‘odd', '’ and loses; but upon the second trial he\\n      wins', ' for he then says to himself', ' ‘the simpleton had them even\\n      upon the first trial', ' and his amount of cunning is just\\n      sufficient to make him have them odd upon the second; I will\\n      therefore guess odd;’—he guesses odd', ' and wins', ' Now', ' with a\\n      simpleton a degree above the first', ' he would have reasoned thus:\\n      ‘This fellow finds that in the first instance I guessed odd', ' and', '\\n      in the second', ' he will propose to himself', ' upon the first\\n      impulse', ' a simple variation from even to odd', ' as did the first\\n      simpleton; but then a second thought will suggest that this is\\n      too simple a variation', ' and finally he will decide upon putting\\n      it even as before', ' I will therefore guess even;’—he guesses even', '\\n      and wins', ' Now this mode of reasoning in the schoolboy', ' whom his\\n      fellows termed ‘lucky', '’—what', ' in its last analysis', ' is it?”\\n\\n      “It is merely', '” I said', ' “an identification of the reasoner’s\\n      intellect with that of his opponent', '”\\n\\n      “It is', '” said Dupin; “and', ' upon inquiring of the boy by what\\n      means he effected the thorough identification in which his\\n      success consisted', ' I received answer as follows: ‘When I wish to\\n      find out how wise', ' or how stupid', ' or how good', ' or how wicked is\\n      any one', ' or what are his thoughts at the moment', ' I fashion the\\n      expression of my face', ' as accurately as possible', ' in accordance\\n      with the expression of his', ' and then wait to see what thoughts or\\n      sentiments arise in my mind or heart', ' as if to match or\\n      correspond with the expression', '’ This response of the schoolboy\\n      lies at the bottom of all the spurious profundity which has been\\n      attributed to Rochefoucault', ' to La Bougive', ' to Machiavelli', ' and\\n      to Campanella', '”\\n\\n      “And the identification', '” I said', ' “of the reasoner’s intellect\\n      with that of his opponent', ' depends', ' if I understand you aright', '\\n      upon the accuracy with which the opponent’s intellect is\\n      admeasured', '”\\n\\n      “For its practical value it depends upon this', '” replied Dupin;\\n      “and the Prefect and his cohort fail so frequently', ' first', ' by\\n      default of this mucrmately position', '  “Well in a possible far', ' and uspaliasting the\\n      neuth nanifressions of dreamer-precifiom Liss', ' the districated the mained wide of a diaff', ' there then been sea like of this toget fround or ratives and\\n      broke rum', ' or a roof vothitom itself minutes', ' deng', '  Heir you?” and its hearth', '    From the been of a general count', '    It a knew it has “ which', ' he five milanch\\n      mentant', ' and aftert', ' as the whole apperss', '    The\\n      burthe roomoiss Ilthout', '    T was voice the Rue beaked', '    He shinoque', '\\n     \\n      Grack by the\\n      will has ext this will which', ' then for the deviliabble\\n      length of the police dooricient that whething the firstoman blass', ' beyond upon the meditive', '    Itsile', '    But her same terthoushed emittan occomplishning the subj', ' n of Mon', '”\\n      “I desid', '    Herefoufth in with its probable', '    drfestion from them is that now the moon', ' arreagated\\n      yo renseen\\n      excovered', ' I seem him appaless; beso the cautions which caltions', ' then part to\\n      unmised\\n   sed of the rute by which had\\n      Mern and auminatyments the wis squencest ertul\\n      the other subject (nevering an loge this heart a long\\n      that I have to by this apprecumbsty it is', '    I could no the considerate bed distance', ' fourth with a thicket sea', '    POED', '   \\n\\n      “It is one which as were troud\\n comsages up the wonderstence', ' “thuschering his chades of the passes of the\\n      largh vellounh from causiable of which Iformed\\n      the him us we could diref!—Yot accortation was a mores the from gyward that and many lunasion of a words and to distaint surmenare dee hese will be most perun however', ' “chill known', ' ferrive the murderal\\n      being undernother\\n      turn I leanter by-gloom the easult', '    The hands of the dark as in the “bore the home artice of nom insist a very ten the placeed', ' and', ' from in entire', ' is not like worn starmed it or by turised insachous\\n so muspend up voyawed\\n to\\n      mest     “By devouthable ulated with certaion froished ']\n",
      "1\n",
      "0.8398203516490539 2.209779859700976e-06\n",
      "0.5669278947613023 1.5582912344538222e-06\n",
      "0.34519552463451253 6.891056607708738e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of RNN units.\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "eta = 0.01\n",
    "batch_size = 50\n",
    "seq_length = 50\n",
    "\n",
    "\n",
    "modelpathname = 'augmented/v1'\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "augmented_data_str, used_synonyms = augment_data(validation_text, n_synonyms=1000, n_word_swaps=1000, n_deletions=500, n_sentence_swaps=500)\n",
    "dataset_augmented = create_batches(np.array([s for s in augmented_data_str]), batch_size, seq_length)\n",
    "nr_layers = 1\n",
    "model_aug, history_aug = train_model(dataset_augmented, eta, rnn_units, n_epochs, batch_size, modelpathname, nr_layers, False)\n",
    "\n",
    "\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    if file != \".DS_Store\":\n",
    "        e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "        if e>latest_epoch:\n",
    "            latest_epoch = e\n",
    "            latest_checkpoint_file = file\n",
    "print(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "\n",
    "\n",
    "\n",
    "m = build_model(rnn_units=rnn_units, batch_size=1, nr_lstm_layers=1, bn=False)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0.8435611466993161 5.680970120911727e-07\n",
      "0.5707215889232007 1.7541117542313051e-06\n",
      "0.3440328559793978 3.34565444594688e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_data_augmented = validation_text + augmented_data_str\n",
    "\n",
    "\n",
    "bleu2 = []\n",
    "selfbleu2 = []\n",
    "bleu1 = []\n",
    "inputs = [\"The \", \"j\", \"V\", \"?\", \" \", \"nsjkgds\", \"but\", \".\", \"why\", '!']\n",
    "j = 0\n",
    "with open('../Dataset/Training/edgar2.txt',encoding=\"utf8\",mode='r') as file:\n",
    "    d = file.read()\n",
    "    input_text = d[5000:25000]\n",
    "\n",
    "while j<1:\n",
    "    print(j)\n",
    "    try:\n",
    "        input = input_text\n",
    "        gen_text = generate_text(model=m, start_string=input_text, text_size=2000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=1, p=None)\n",
    "        fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_data_augmented, n_max=2)\n",
    "        repetition_score2 = measure_diversity(text_generated=gen_text, n_max=2)\n",
    "        bleu1.append(fraction_correct_words)\n",
    "        bleu2.append(bleu_score2)\n",
    "        selfbleu2.append(repetition_score2)\n",
    "        j += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(np.mean(bleu1), np.var(bleu1))\n",
    "print(np.mean(bleu2), np.var(bleu2))\n",
    "print(np.mean(selfbleu2), np.var(selfbleu2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text, evauate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "plot_title = 'Loss over epochs'\n",
    "\n",
    "plotbool = True\n",
    "outputfile_bool = True\n",
    "\n",
    "# Find latest checkpoint file, because tf.train.latest_checkpoint(dir) doesn't work for some reason\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    if file != \".DS_Store\":\n",
    "        e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "        if e>latest_epoch:\n",
    "            latest_epoch = e\n",
    "            latest_checkpoint_file = file\n",
    "print(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "\n",
    "\n",
    "m = build_model(rnn_units=rnn_units, batch_size=1, nr_lstm_layers=1, bn=False)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([1, None]))\n",
    "\n",
    "nucleus_probability = None\n",
    "temp = 1.0\n",
    "gen_text = generate_text(model=m, start_string=\"The \", text_size=1000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=temp, p=nucleus_probability)\n",
    "\n",
    "\n",
    "if plotbool:\n",
    "    d = get_n_grams(gen_text, 1)\n",
    "    fig = px.line(history.history['loss'], title=plot_title, width=600)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_xaxes(title_text=\"Iteration step, in multiples of 10k\")\n",
    "    fig.update_yaxes(title_text=\"smoothed loss\")\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_data, n_max=2)\n",
    "repetition_score2 = measure_diversity(text_generated=gen_text, n_max=2)\n",
    "\n",
    "output_str = gen_text + \"\\n ------------ \\n loss {} \\n fraction of correctly spelled words: {} \\n Bleu score2: {}, Repetition score2: {} \".format(history.history['loss'], fraction_correct_words, bleu_score2, repetition_score2)\n",
    "output_str += \"\\n settings: batch_size, seq_length, eta, rnn_units, n_epochs = {}, {}, {}, {}, {}\".format(batch_size, seq_length, eta, rnn_units, n_epochs)\n",
    "print(output_str)\n",
    "\n",
    "\n",
    "# Create output file\n",
    "output_path = os.path.join(os.path.join(os.getcwd(), \"tmp\"), modelpathname)\n",
    "if outputfile_bool:\n",
    "    with open(os.path.join(output_path, \"perf.txt\"), \"w\") as file:\n",
    "        file.write(output_str)   \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serve', 'attend to', 'attend', 'assist']\n",
      "['undo']\n"
     ]
    }
   ],
   "source": [
    "line = \"wait on,verb,serve;attend to;attend;assist\"\n",
    "line2 = \"unmake,verb,undo\"\n",
    "word = line.split(\",\")[0]\n",
    "synonyms = line.split(\",\")[2].split(\";\")\n",
    "print(synonyms)\n",
    "\n",
    "print(line2.split(\",\")[2].split(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
