{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2424 Project in Deep Learning in Data Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "book_data = np.array(load_data('../Dataset/Training/vol1.txt'))\n",
    "book_chars = np.unique(book_data)\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "m = 100\n",
    "eta = 0.1\n",
    "seq_length = 25\n",
    "sig = 0.01\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m, k, eta, seq_length, sig)\n",
    "h0 = np.zeros((m, 1))\n",
    "epochs = 10\n",
    "losslist = []\n",
    "iteration = 0\n",
    "smoothloss = 0\n",
    "sentences = []\n",
    "for i in range(epochs):\n",
    "    rnn.hprev = np.zeros((m, 1))\n",
    "    for e in range(0, book_data.shape[0]-seq_length-1, seq_length):\n",
    "        X_chars = book_data[e:e+seq_length]\n",
    "        Y_chars = book_data[e+1:e+seq_length+1]\n",
    "        X = one_hot_encoding(X_chars, char_to_ind, k)\n",
    "        Y = one_hot_encoding(Y_chars, char_to_ind, k)\n",
    "        loss = rnn.adagrad(X, Y, h0)\n",
    "        if smoothloss == 0:\n",
    "            smoothloss = loss\n",
    "        smoothloss = 0.999*smoothloss + 0.001*loss\n",
    "     \n",
    "        if iteration % 10000 == 0:\n",
    "            print('Iteration: {}, Loss: {}'.format(iteration, smoothloss))\n",
    "            y = rnn.synthetize(rnn.hprev, X[:, 0], 200)\n",
    "            sentence = one_hot_decoding(y, ind_to_char)\n",
    "            print(sentence)\n",
    "            sentences.append(sentence)\n",
    "            losslist.append(smoothloss)\n",
    "        iteration += 1\n",
    "        if iteration>400001:\n",
    "            break\n",
    "    if iteration>400001:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "book_data = np.array(load_data('../Dataset/Training/vol1.txt',remove_footnotes=False))\n",
    "print(book_data)\n",
    "book_chars = np.unique(book_data)\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "batch_size = 50\n",
    "seq_length = 50\n",
    "\n",
    "book_data_ind = np.array([char_to_ind[c] for c in book_data])\n",
    "print(char_to_ind)\n",
    "# Split data into sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(book_data_ind)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(data):\n",
    "    input = data[:-1]\n",
    "    target = data[1:]\n",
    "    return input, target\n",
    "\n",
    "# Split data into X, Y\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# This organizes the data into groups of sequences. batch_size denotes the number of sequences in a batch, and seq_length denotes the number of characters in a sequence.\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(Y_true,Y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true=Y_true, y_pred=Y_pred, from_logits=True)\n",
    "\n",
    "# Length of the vocabulary in chars.\n",
    "vocab_size = len(book_chars)\n",
    "\n",
    "# The embedding dimension.\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units.\n",
    "rnn_units = 200\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    m = tf.keras.Sequential()\n",
    "    m.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    m.add(tf.keras.layers.BatchNormalization(synchronized=True))\n",
    "    m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    m.add(tf.keras.layers.Dense(vocab_size))\n",
    "    #m.summary()\n",
    "    return m\n",
    "\n",
    "m = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "\n",
    "# try the model\n",
    "for i_ex, t_ex in dataset.take(1):\n",
    "    example_pred = m(i_ex)  # this step builds the model\n",
    "    sampled_indices = tf.random.categorical(logits=example_pred[0], num_samples=1)\n",
    "    sample_ind_1d = tf.squeeze(input=sampled_indices, axis=-1).numpy()\n",
    "print('Input:\\n', repr(''.join([ind_to_char[c] for c in i_ex.numpy()[0]])))\n",
    "print('Next char prediction:\\n', repr(''.join([ind_to_char[c] for c in sample_ind_1d])))  \n",
    "\n",
    "# Directory where the checkpoints will be saved.\n",
    "current_dir_path = os.getcwd()\n",
    "checkpoint_dir = current_dir_path + \"\\\\tmp\\\\checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch:02d}.hdf5')\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True, \n",
    "    save_best_onl =True\n",
    ")\n",
    "\n",
    "\n",
    "# Specify update rule and compile model\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "m.compile(optimizer=adam_opt, loss=loss)\n",
    "\n",
    "# train\n",
    "history = m.fit(x=dataset, epochs=2, callbacks=[checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint file, because tf.train.latest_checkpoint(dir) doesn't work for some reason\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "    if e>latest_epoch:\n",
    "        latest_epoch = e\n",
    "        latest_checkpoint_file = file\n",
    "print(latest_checkpoint_file)\n",
    "\n",
    "latest_checkpoint_file = \"ckpt_02.hdf5\"\n",
    "\n",
    "simplified_batch_size = 1\n",
    "m = build_model(vocab_size, embedding_dim, rnn_units, simplified_batch_size)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([simplified_batch_size, None]))\n",
    "\n",
    "def generate_text(model, start_string, text_size):\n",
    "    # Convert start string to numbers\n",
    "    input_indices = tf.expand_dims([char_to_ind[s] for s in start_string], 0)\n",
    "\n",
    "    generated_text = \"\"\n",
    "    model.reset_states()\n",
    "    for i in range(text_size):\n",
    "        predictions = model(input_indices)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Sample a new character based on the log probability distribution in 'predictions'\n",
    "        sampled_id = tf.random.categorical(\n",
    "        predictions,\n",
    "        num_samples=1\n",
    "        )[-1,0].numpy()\n",
    "\n",
    "        # Use sampled char as input for next iteration\n",
    "        input_indices = tf.expand_dims([sampled_id], 0)\n",
    "        generated_text += ind_to_char[sampled_id]\n",
    "\n",
    "    return start_string + generated_text\n",
    "\n",
    "gen_text = generate_text(model=m, start_string=u\"Water\", text_size=200)\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for e in range(0, book_data.shape[0]-seq_length-1, seq_length):\n",
    "    X_chars = book_data[e:e+seq_length]\n",
    "    Y_chars = book_data[e+1:e+seq_length+1]\n",
    "    #X.append(X_chars)\n",
    "    #X.append(Y_chars)\n",
    "    X.append(one_hot_encoding(X_chars, char_to_ind, k))\n",
    "    Y.append(one_hot_encoding(Y_chars, char_to_ind, k))\n",
    "    #hist = model.fit(X,Y)\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "# Length of the vocabulary in chars.\n",
    "vocab_size = len(book_chars)\n",
    "\n",
    "# The embedding dimension.\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units.\n",
    "rnn_units = 1024\n",
    "batch_size = 1\n",
    "m = tf.keras.models.Sequential()\n",
    "m.add(tf.keras.layers.LSTM(units=50, return_sequences=True, input_shape=(X)))\n",
    "m.add(tf.keras.layers.Dropout(0.2))\n",
    "m.add(tf.keras.layers.LSTM(units=50))\n",
    "m.add(tf.keras.layers.Dropout(0.2))\n",
    "m.add(tf.keras.layers.Dense(units=1))\n",
    "m.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "print(X[0].shape)\n",
    "history = m.fit(X,Y, epochs=1, batch_size=1, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
