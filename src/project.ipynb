{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2424 Project in Deep Learning in Data Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "book_data = np.array(load_data('../Dataset/Training/vol1.txt'))\n",
    "book_chars = np.unique(book_data)\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "m = 100\n",
    "eta = 0.1\n",
    "seq_length = 25\n",
    "sig = 0.01\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m, k, eta, seq_length, sig)\n",
    "h0 = np.zeros((m, 1))\n",
    "epochs = 10\n",
    "losslist = []\n",
    "iteration = 0\n",
    "smoothloss = 0\n",
    "sentences = []\n",
    "for i in range(epochs):\n",
    "    rnn.hprev = np.zeros((m, 1))\n",
    "    for e in range(0, book_data.shape[0]-seq_length-1, seq_length):\n",
    "        X_chars = book_data[e:e+seq_length]\n",
    "        Y_chars = book_data[e+1:e+seq_length+1]\n",
    "        X = one_hot_encoding(X_chars, char_to_ind, k)\n",
    "        Y = one_hot_encoding(Y_chars, char_to_ind, k)\n",
    "        loss = rnn.adagrad(X, Y, h0)\n",
    "        if smoothloss == 0:\n",
    "            smoothloss = loss\n",
    "        smoothloss = 0.999*smoothloss + 0.001*loss\n",
    "     \n",
    "        if iteration % 10000 == 0:\n",
    "            print('Iteration: {}, Loss: {}'.format(iteration, smoothloss))\n",
    "            y = rnn.synthetize(rnn.hprev, X[:, 0], 200)\n",
    "            sentence = one_hot_decoding(y, ind_to_char)\n",
    "            print(sentence)\n",
    "            sentences.append(sentence)\n",
    "            losslist.append(smoothloss)\n",
    "        iteration += 1\n",
    "        if iteration>400001:\n",
    "            break\n",
    "    if iteration>400001:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['´' '╗' '┐' ... '\\n' '\\n' '\\n']\n",
      "{'\\n': 0, ' ': 1, '!': 2, '#': 3, '$': 4, '&': 5, '(': 6, ')': 7, '*': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, ';': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, '[': 52, ']': 53, '_': 54, 'a': 55, 'b': 56, 'c': 57, 'd': 58, 'e': 59, 'f': 60, 'g': 61, 'h': 62, 'i': 63, 'j': 64, 'k': 65, 'l': 66, 'm': 67, 'n': 68, 'o': 69, 'p': 70, 'q': 71, 'r': 72, 's': 73, 't': 74, 'u': 75, 'v': 76, 'w': 77, 'x': 78, 'y': 79, 'z': 80, '~': 81, '£': 82, 'ª': 83, '¬': 84, '®': 85, '´': 86, '»': 87, '¿': 88, 'Á': 89, 'Â': 90, 'Ç': 91, 'Ô': 92, 'Ö': 93, 'Ø': 94, 'á': 95, 'í': 96, 'ó': 97, 'ö': 98, 'ÿ': 99, '│': 100, '┐': 101, '├': 102, '┤': 103, '┬': 104, '╗': 105, '░': 106, '▓': 107}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_data_filename = '../Dataset/Training/edgar.txt'\n",
    "book_data = np.array(load_data(training_data_filename,remove_footnotes=False))\n",
    "with open(training_data_filename,encoding='cp850',mode='r') as f:\n",
    "   words = f.read().split()\n",
    "with open(training_data_filename,encoding='cp850',mode='r') as file:\n",
    "    validation_text = file.read()\n",
    "print(book_data)\n",
    "book_chars = np.unique(book_data)\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 50 # [25, 75, 125]\n",
    "seq_length = 50 # [25, 75, 125]\n",
    "learningrate = 0.1 # [0.1, 0.01, 0.001]\n",
    "book_data_ind = np.array([char_to_ind[c] for c in book_data])\n",
    "print(char_to_ind)\n",
    "# Split data into sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(book_data_ind)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(data):\n",
    "    input = data[:-1]\n",
    "    target = data[1:]\n",
    "    return input, target\n",
    "\n",
    "# Split data into X, Y\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# This organizes the data into groups of sequences. batch_size denotes the number of sequences in a batch, and seq_length denotes the number of characters in a sequence.\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " 'ime enough\\n      ÔÇÿto throw the body into the riv'\n",
      "Next char prediction:\n",
      " 'X8wx]F6_dá»Z5LS[OX2YdrÖ├pft░!(v:*TLWKZ-3uK4($~├VF6'\n",
      "226/226 [==============================] - 96s 408ms/step - loss: 1.9734\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelpathname = 'v1'\n",
    "def loss(Y_true,Y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true=Y_true, y_pred=Y_pred, from_logits=True)\n",
    "\n",
    "# Length of the vocabulary in chars.\n",
    "vocab_size = len(book_chars)\n",
    "\n",
    "# The embedding dimension.\n",
    "embedding_dim = 256 \n",
    "\n",
    "# Number of RNN units.\n",
    "u = [50, 100, 200, 500, 1000]\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    m = tf.keras.Sequential()\n",
    "    m.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    m.add(tf.keras.layers.BatchNormalization(synchronized=True))\n",
    "    m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    m.add(tf.keras.layers.Dense(vocab_size))\n",
    "    #m.summary()\n",
    "    return m\n",
    "\n",
    "m = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "\n",
    "# try the model\n",
    "for i_ex, t_ex in dataset.take(1):\n",
    "    example_pred = m(i_ex)  # this step builds the model\n",
    "    sampled_indices = tf.random.categorical(logits=example_pred[0], num_samples=1)\n",
    "    sample_ind_1d = tf.squeeze(input=sampled_indices, axis=-1).numpy()\n",
    "print('Input:\\n', repr(''.join([ind_to_char[c] for c in i_ex.numpy()[0]])))\n",
    "print('Next char prediction:\\n', repr(''.join([ind_to_char[c] for c in sample_ind_1d])))  \n",
    "\n",
    "# Directory where the checkpoints will be saved.\n",
    "current_dir_path = os.getcwd()\n",
    "checkpoint_dir = current_dir_path + \"\\\\tmp\\\\\" + modelpathname+ \"\\\\training_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch:02d}.hdf5')\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True, \n",
    "    save_best_onl =True\n",
    ")\n",
    "\n",
    "\n",
    "# Specify update rule and compile model\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=learningrate)\n",
    "m.compile(optimizer=adam_opt, loss=loss)\n",
    "\n",
    "# train\n",
    "history = m.fit(x=dataset, epochs=n_epochs, callbacks=[checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_01.hdf5\n",
      "Water . Ocountr-bullÔÇöGoether ouralo,, t lls, up elly upinon lapeare mauleÔÇö_serec.ÔÇö it ÔÇ£Floun froke, Mand tirt. fanc. hose jurn eoks by the soule ony\n",
      " whey hily soup re dentin-yout were by ano he fre obe tory no\n",
      " He 1re opurigion,yele dowy dedk eemant        4   ipodaly In unsoreatel-an uno tince-intowh ire of furautedile; ambody\n",
      "fore\n",
      " Iiod the in le neotery ised. digalyÔÇes of ancoung mistous dan\n",
      " whome anchiwele usterouly, body, ÔÇöSurfan, and\n",
      "    nouag ounidke finio_ of all be\n",
      "    frome he ammosk of anture gal, muckineigs. That the nif on cont  he hers, hut have anrady!siagn to apsion upigh.ÔÇØ\n",
      "\n",
      ", weke ne or\n",
      "_ is dere guir toulu_\n",
      " le be trie ri grieve\n",
      " in attere we cur toge, o weroy wenory in uphe tlat. antour.) ful wo. jume by ut bose. of I Vure looke fro matlat the le norighouly,aut uf no suro surace of haghe, is of norinous it to wered an mar-abos-tertable tre to logy Berifid mame tho 2rpin wilo le be Madue, ony withouly curm bore an, he stals ere de tore ony it be toumin to the\n"
     ]
    }
   ],
   "source": [
    "# Find latest checkpoint file, because tf.train.latest_checkpoint(dir) doesn't work for some reason\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "    if e>latest_epoch:\n",
    "        latest_epoch = e\n",
    "        latest_checkpoint_file = file\n",
    "print(latest_checkpoint_file)\n",
    "\n",
    "latest_checkpoint_file = \"ckpt_01.hdf5\"\n",
    "\n",
    "simplified_batch_size = 1\n",
    "m = build_model(vocab_size, embedding_dim, rnn_units, simplified_batch_size)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([simplified_batch_size, None]))\n",
    "\n",
    "gen_text = generate_text(m, \"Water \", 200, char_to_ind, ind_to_char)\n",
    "print(gen_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history['loss'])\n",
    "d = get_n_grams(gen_text, 1)\n",
    "fig = px.line(history.history['loss'], title='Loss over epochs')\n",
    "#fig = px.scatter(history.history['loss'], title='Loss over epochs')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "correctwords = 0\n",
    "for word in d: \n",
    "    if word in words: \n",
    "        #print(word)\n",
    "        correctwords += 1\n",
    "\n",
    "print(\"Correct % words: {}\".format(correctwords/len(d)))\n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "fraction_correct_words, bleu_score = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=2)\n",
    "repetition_score = measure_diversity(text_generated=gen_text, n_max=2)\n",
    "print(\"fraction of correctly spelled words: {} \\n Bleu score: {} \\n Repetition score: {}\".format(fraction_correct_words, bleu_score, repetition_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
