{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2424 Project in Deep Learning in Data Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tqdm\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filename = '../Dataset/Training/edgar.txt'\n",
    "book_data = np.array(load_data(training_data_filename))\n",
    "book_chars = np.unique(book_data)\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "m = 100\n",
    "eta = 0.1\n",
    "seq_length = 25\n",
    "sig = 0.01\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m, k, eta, seq_length, sig)\n",
    "h0 = np.zeros((m, 1))\n",
    "max_iter = 200000\n",
    "epochs = 10\n",
    "smoothloss_list = []\n",
    "loss_list = []\n",
    "iteration = 0\n",
    "smoothloss = 0\n",
    "sentences = []\n",
    "for i in range(epochs):\n",
    "    rnn.hprev = np.zeros((m, 1))\n",
    "    for e in range(0, book_data.shape[0]-seq_length-1, seq_length):\n",
    "        X_chars = book_data[e:e+seq_length]\n",
    "        Y_chars = book_data[e+1:e+seq_length+1]\n",
    "        X = one_hot_encoding(X_chars, char_to_ind, k)\n",
    "        Y = one_hot_encoding(Y_chars, char_to_ind, k)\n",
    "        loss = rnn.adagrad(X, Y, h0, iteration)\n",
    "        if smoothloss == 0:\n",
    "            smoothloss = loss\n",
    "        smoothloss = 0.999*smoothloss + 0.001*loss\n",
    "     \n",
    "        if iteration % 10000 == 1:\n",
    "            print('Iteration: {}, Loss: {} '.format(iteration, smoothloss))\n",
    "            y = rnn.synthetize(rnn.hprev, X[:, 0], 200)\n",
    "            sentence = one_hot_decoding(y, ind_to_char)\n",
    "            print(sentence + \"\\n\")\n",
    "            #sentences.append(sentence)\n",
    "            smoothloss_list.append(smoothloss)\n",
    "            loss_list.append(loss)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration>max_iter:\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(training_data_filename,encoding='utf-8-sig',mode='r') as file:\n",
    "    validation_text = file.read()\n",
    "start_char = \"T\"\n",
    "start_char_onehot = one_hot_encoding(start_char, char_to_ind, k)\n",
    "generated_text_vanilla_onehot = rnn.synthetize(rnn.hprev, start_char_onehot, 1000)\n",
    "generated_text_vanilla = start_char + one_hot_decoding(generated_text_vanilla_onehot, ind_to_char)\n",
    "print(generated_text_vanilla)\n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "nmax = 4\n",
    "fraction_correct_words, bleu_score = measure_bleu(text_generated=generated_text_vanilla, text_val=validation_text, n_max=nmax)\n",
    "repetition_score = measure_diversity(text_generated=generated_text_vanilla, n_max=nmax)\n",
    "print(\"\\n loss function\", loss_list)\n",
    "print(\"\\n fraction of correctly spelled words: {} \\n Bleu score: {} \\n Repetition score: {}\".format(fraction_correct_words, bleu_score, repetition_score))\n",
    "\n",
    "\n",
    "\n",
    "fig = px.line(smoothloss_list, title='Smoothed loss over epochs', width=600)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_yaxes(title_text=\"smoothed loss\")\n",
    "fig.update_xaxes(title_text=\"iteration step, in multiples of 10k\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1_filename = '../Dataset/Training/edgar1.txt'\n",
    "vol2_filename = '../Dataset/Training/edgar2.txt'\n",
    "vol3_filename = '../Dataset/Training/edgar3.txt'\n",
    "vol4_filename = '../Dataset/Training/edgar4.txt'\n",
    "vol5_filename = '../Dataset/Training/edgar5.txt'\n",
    "\n",
    "encoding =\"utf8\"\n",
    "\n",
    "book_data_vol1 = np.array(load_data(vol1_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol2 = np.array(load_data(vol2_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol3 = np.array(load_data(vol3_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol4 = np.array(load_data(vol4_filename, remove_footnotes=False, encoding=encoding))\n",
    "book_data_vol5 = np.array(load_data(vol5_filename, remove_footnotes=False, encoding=encoding))\n",
    "\n",
    "\n",
    "ngram_validation_text = \"\"\n",
    "for filename in [vol1_filename, vol2_filename, vol3_filename, vol4_filename, vol5_filename]:\n",
    "    with open(filename, encoding=encoding,mode='r') as file:\n",
    "        ngram_validation_text += file.read()\n",
    "\n",
    "with open('../Dataset/Training/synonyms.csv', encoding=encoding, mode=\"r\") as file:\n",
    "    all_synonyms = file.read()   \n",
    "\n",
    "\n",
    "all_book_data = np.concatenate(([s for s in ngram_validation_text], np.array([s for s in all_synonyms])))\n",
    "book_chars = np.unique(all_book_data)\n",
    "\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "\n",
    "vocab_size = len(book_chars)\n",
    "embedding_dim = 256 \n",
    "\n",
    "\n",
    "def create_batches(data, batch_size, seq_length):\n",
    "    book_data_ind = np.array([char_to_ind[c] for c in data])\n",
    "    # Split data into sequences\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(book_data_ind)\n",
    "    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "    # Split data into X, Y\n",
    "    dataset = sequences.map(split_input_target)\n",
    "\n",
    "    # This organizes the data into groups of sequences. batch_size denotes the number of sequences in a batch, and seq_length denotes the number of characters in a sequence.\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "def build_model(rnn_units, batch_size, nr_lstm_layers, bn, bpe_weights=None):\n",
    "    m = tf.keras.Sequential()\n",
    "    if weights is None:\n",
    "        m.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    else:\n",
    "        m.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None], embeddings_initializer=tf.keras.initializers.Constant(bpe_weights)))\n",
    "    for i in range(nr_lstm_layers):\n",
    "        m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    if bn:\n",
    "        m.add(tf.keras.layers.BatchNormalization())\n",
    "    m.add(tf.keras.layers.Dense(vocab_size))\n",
    "    return m\n",
    "\n",
    "\n",
    "def get_latest_checkpoint_file(modelpathname):\n",
    "    latest_epoch = 0\n",
    "    latest_checkpoint_file = \"\"\n",
    "    checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "    for file in os.listdir(checkpoint_dir):\n",
    "        if file != \".DS_Store\":\n",
    "            e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "            if e>latest_epoch:\n",
    "                latest_epoch = e\n",
    "                latest_checkpoint_file = file\n",
    "    return os.path.join(checkpoint_dir, latest_checkpoint_file)\n",
    "\n",
    "\n",
    "def train_normal_model(dataset_train, dataset_val, eta, rnn_units, n_epochs, batch_size, output_filename, nr_lstm_layers=1, bn=False, bpe_weights=None):\n",
    "    m = build_model(rnn_units, batch_size, nr_lstm_layers, bn, bpe_weights)\n",
    "\n",
    "    for i_ex, t_ex in dataset_train.take(1):\n",
    "        example_pred = m(i_ex)  # this step builds the model\n",
    "\n",
    "    # Specify update rule and compile model\n",
    "    adam_opt = tf.keras.optimizers.Adam(learning_rate=eta)\n",
    "    loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    m.compile(optimizer=adam_opt, loss=loss_func)\n",
    "\n",
    "    # Configure checkpoints\n",
    "    current_dir_path = os.getcwd()\n",
    "    checkpoint_dir = os.path.join(os.path.join(os.path.join(current_dir_path, \"tmp\"), output_filename), \"training_checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch:02d}.hdf5')\n",
    "    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True, save_best_only=False)\n",
    "\n",
    "    # train\n",
    "    if dataset_val:\n",
    "        history = m.fit(x=dataset_train, epochs=n_epochs, validation_data=dataset_val, callbacks=[checkpoint_callback])\n",
    "    else:\n",
    "        history = m.fit(x=dataset_train, epochs=n_epochs, callbacks=[checkpoint_callback])\n",
    "    return m, history \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test character based LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn_units = 50\n",
    "n_epochs = 5\n",
    "eta = 0.01\n",
    "batch_size = 50\n",
    "seq_length = 50\n",
    "\n",
    "modelpathname = 'v7_1'\n",
    "nr_layers = 1\n",
    "\n",
    "#augmented_data_str, used_synonyms = augment_data(validation_text, n_synonyms=1000, n_word_swaps=1000, n_deletions=500, n_sentence_swaps=500)\n",
    "dataset_train = create_batches(book_data_vol1, batch_size, seq_length)\n",
    "dataset_val = create_batches(book_data_vol2, batch_size, seq_length)\n",
    "\n",
    "use_bn = False\n",
    "model, history= train_normal_model(dataset_train, dataset_val, eta, rnn_units, n_epochs, batch_size, modelpathname, nr_layers, use_bn)\n",
    "\n",
    "latest_checkpoint_file = get_latest_checkpoint_file(modelpathname)\n",
    "m = build_model(rnn_units=rnn_units, batch_size=1, nr_lstm_layers=nr_layers, bn=use_bn)\n",
    "m.load_weights(latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([1, None]))\n",
    "\n",
    "inputt = ngram_validation_text[1000:1200]\n",
    "validation_text = ngram_validation_text\n",
    "gen_text = generate_text(model=m, start_string=inputt, text_size=1000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=1, p=None)\n",
    "nmax = 2\n",
    "fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=nmax)\n",
    "repetition_score2 = measure_diversity(text_generated=gen_text, n_max=nmax)\n",
    "\n",
    "print(gen_text)\n",
    "print(fraction_correct_words, bleu_score2, repetition_score2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BPE Tokenization\"\"\"\n",
    "\n",
    "def compute_pair_count(splits, word_count):\n",
    "    pair_count = {}\n",
    "    for word, count in word_count.items():\n",
    "        split = splits[word]\n",
    "      \n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            if pair in pair_count:\n",
    "                pair_count[pair] += count\n",
    "            else:\n",
    "                pair_count[pair] = count\n",
    "    return pair_count\n",
    "\n",
    "def merge_pair(a, b, splits,word_count):\n",
    "    for word in word_count:\n",
    "     \n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "    \n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        splits[word] = split\n",
    "   \n",
    "    return splits\n",
    "\n",
    "def BPE(text, vocab_size,word_count):\n",
    "    merges = {}\n",
    "    book_chars = np.unique(text)\n",
    "    splits =  {word: [c for c in word] for word in word_count.keys()}\n",
    "    while len(book_chars) < vocab_size:\n",
    "        pair_count = compute_pair_count(splits, word_count)\n",
    "        best_pair = \"\"\n",
    "        max_count = None\n",
    "        for pair, count in pair_count.items():\n",
    "            if max_count is None or max_count < count:\n",
    "                best_pair = pair\n",
    "                max_count = count\n",
    "        splits = merge_pair(*best_pair, splits, word_count)\n",
    "        merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "        book_chars = np.append(book_chars,best_pair[0] + best_pair[1])\n",
    "    return splits, merges, book_chars\n",
    "\n",
    "def tokenize(words, merges):\n",
    "    word_count = {}\n",
    "    for word in words:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "    \n",
    "    splits =  [[c for c in word] for word in word_count.keys()]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, num_ns):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots\n",
    " \n",
    "\n",
    "def train_word2Vec(dataset_train, dataset_val, vocab_size, num_ns, n_epochs):\n",
    "    word2vec = Word2Vec(vocab_size, embedding_dim, num_ns)\n",
    "    word2vec.compile(optimizer='adam',\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "    if dataset_val:\n",
    "      word2vec.fit(dataset_train, validation_data=dataset_val, epochs=n_epochs)#, callbacks=[tensorboard_callback])\n",
    "    else:\n",
    "      word2vec.fit(dataset_train, epochs=n_epochs)#, callbacks=[tensorboard_callback])\n",
    "    return word2vec\n",
    "\n",
    "\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data for BPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11150/11150 [00:28<00:00, 387.46it/s]\n",
      "100%|██████████| 11800/11800 [00:31<00:00, 373.10it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc = \"utf8\"\n",
    "_, words1 = load_data1(vol1_filename, encoding=enc)\n",
    "_, words2 = load_data1(vol2_filename, encoding=enc)\n",
    "_, words3 = load_data1(vol3_filename, encoding=enc)\n",
    "_, words4 = load_data1(vol4_filename, encoding=enc)\n",
    "_, words5 = load_data1(vol5_filename, encoding=enc)\n",
    "\n",
    "word_count = {}\n",
    "all_words = words1 #np.concatenate((words1, words2, words3, words4, words5))\n",
    "for word in all_words:\n",
    "    if word in word_count:\n",
    "        word_count[word] += 1\n",
    "    else:\n",
    "        word_count[word] = 1\n",
    "\n",
    "\n",
    "splits, merges, book_chars = BPE(all_book_data, 200, word_count)\n",
    "tokenized_text = tokenize(all_words, merges)\n",
    "\n",
    "eta = 0.01 # Best = 0.01\n",
    "batch_size = 50 # Best = 25\n",
    "seq_length = 50 # Best = 125\n",
    "num_ns = 4\n",
    "\n",
    "#Initalize the dataset that will be used for LSTM network\n",
    "dataset_train = create_batches(book_data_vol1, batch_size, seq_length)\n",
    "dataset_val = create_batches(book_data_vol2, batch_size, seq_length)\n",
    "seq_list_train = []\n",
    "for batch in dataset_train:\n",
    "   for seq in batch[0]:\n",
    "       seq_list_train.append(seq.numpy())\n",
    "\n",
    "# seq_list_val = []\n",
    "# for batch in dataset_val:\n",
    "#     for seq in batch[0]:\n",
    "#         seq_list_val.append(seq.numpy())     \n",
    "\n",
    "# Generate training and validation data for word2vec using the sequences from the LSTM dataset\n",
    "targets, contexts, labels = generate_training_data(seq_list_train, 2, num_ns, vocab_size)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset_train = dataset_train.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# # Generate training data for word2vec using the sequences from the LSTM dataset\n",
    "# targets, contexts, labels = generate_training_data(seq_list_val, 2, num_ns, vocab_size)\n",
    "# dataset_val = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "# dataset_val = dataset_val.shuffle(10000).batch(batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word2vec weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1850/1850 [==============================] - 5s 2ms/step - loss: 0.4105 - accuracy: 0.8647\n",
      "Epoch 2/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3414 - accuracy: 0.8791\n",
      "Epoch 3/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3363 - accuracy: 0.8806\n",
      "Epoch 4/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3333 - accuracy: 0.8818\n",
      "Epoch 5/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3311 - accuracy: 0.8824\n",
      "Epoch 6/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3296 - accuracy: 0.8836\n",
      "Epoch 7/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3287 - accuracy: 0.8835\n",
      "Epoch 8/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3278 - accuracy: 0.8832\n",
      "Epoch 9/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3272 - accuracy: 0.8834\n",
      "Epoch 10/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3268 - accuracy: 0.8842\n",
      "Epoch 11/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3261 - accuracy: 0.8842\n",
      "Epoch 12/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3259 - accuracy: 0.8842\n",
      "Epoch 13/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3257 - accuracy: 0.8839\n",
      "Epoch 14/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3256 - accuracy: 0.8842\n",
      "Epoch 15/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3255 - accuracy: 0.8835\n",
      "Epoch 16/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3255 - accuracy: 0.8837\n",
      "Epoch 17/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3252 - accuracy: 0.8841\n",
      "Epoch 18/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3250 - accuracy: 0.8843\n",
      "Epoch 19/20\n",
      "1850/1850 [==============================] - 4s 2ms/step - loss: 0.3250 - accuracy: 0.8842\n",
      "Epoch 20/20\n",
      "1850/1850 [==============================] - 5s 2ms/step - loss: 0.3250 - accuracy: 0.8843\n",
      "Epoch 1/5\n",
      "223/223 [==============================] - 23s 89ms/step - loss: 2.6029 - val_loss: 2.1879\n",
      "Epoch 2/5\n",
      "223/223 [==============================] - 20s 86ms/step - loss: 2.0564 - val_loss: 1.9674\n",
      "Epoch 3/5\n",
      "223/223 [==============================] - 20s 85ms/step - loss: 1.9102 - val_loss: 1.8814\n",
      "Epoch 4/5\n",
      "223/223 [==============================] - 20s 87ms/step - loss: 1.8267 - val_loss: 1.8230\n",
      "Epoch 5/5\n",
      "223/223 [==============================] - 21s 89ms/step - loss: 1.7740 - val_loss: 1.7816\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nr_layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m model, history \u001b[39m=\u001b[39m train_normal_model(dataset_train, dataset_val, eta, rnn_units, n_epochs, batch_size, modelpathname, \u001b[39m2\u001b[39m, use_bn, weights1)\n\u001b[0;32m     25\u001b[0m latest_checkpoint_file \u001b[39m=\u001b[39m get_latest_checkpoint_file(modelpathname)\n\u001b[1;32m---> 26\u001b[0m m \u001b[39m=\u001b[39m build_model(rnn_units\u001b[39m=\u001b[39mrnn_units, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, nr_lstm_layers\u001b[39m=\u001b[39mnr_layers, bn\u001b[39m=\u001b[39muse_bn, bpe_weights\u001b[39m=\u001b[39mweights1)\n\u001b[0;32m     27\u001b[0m m\u001b[39m.\u001b[39mload_weights(latest_checkpoint_file)\n\u001b[0;32m     28\u001b[0m m\u001b[39m.\u001b[39mbuild(tf\u001b[39m.\u001b[39mTensorShape([\u001b[39m1\u001b[39m, \u001b[39mNone\u001b[39;00m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nr_layers' is not defined"
     ]
    }
   ],
   "source": [
    "modelpathname = 'v7_1'\n",
    "\n",
    "#Train word2vec model and get the weights\n",
    "\n",
    "#word2vec = train_word2Vec(dataset_train, dataset_val, vocab_size, num_ns, 20)\n",
    "word2vec = train_word2Vec(dataset_train, None, vocab_size, num_ns, 10)\n",
    "\n",
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "\n",
    "#print(weights)\n",
    "pd.DataFrame(weights).to_csv(\"weights_word2vec.csv\")\n",
    "word2vec.save_weights(\"word2vec_weights_model.h5\")\n",
    "weights1 = pd.read_csv(\"weights_word2vec.csv\", index_col=0).to_numpy()\n",
    "\n",
    "\n",
    "# Train normal LSTM model with bpe embedding\n",
    "n_epochs = 5\n",
    "rnn_units = 50\n",
    "use_bn = False\n",
    "dataset_train = create_batches(book_data_vol1, batch_size, seq_length)\n",
    "dataset_val = create_batches(book_data_vol2, batch_size, seq_length)\n",
    "nr_layers = 2\n",
    "model, history = train_normal_model(dataset_train, dataset_val, eta, rnn_units, n_epochs, batch_size, modelpathname, nr_layers, use_bn, weights1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pidereds wh’ when esterst and mymestiom\n",
      "      died. But on to, to sceld, in\n",
      "      thoterhe shile! I gaverer iken meriol yeat whais    romentinus up\n",
      "      the shagent-aty that obrecples. ‘Lisher caul had mush to to it, thene the cherrumise obver, with whos “You the Ξo dorictin poafiaphere. It nlaidind my ut this nowelllition to the conntais. But in\n",
      "      to mannoul d, the stidentered his defical a fear of linht mystir specesering ext untimy, and he      larred of the cietint chared dy me that to fories to\n",
      "      my\n",
      "      the and yout darratore before abition)alg, entepition, one had obles the fangn, that with the all in be\n",
      "      cownersed, left allg,\n",
      "      when neashery while furtions.\n",
      "\n",
      "       hear whowether, “Applaiginht Dugn lademation howe of consall\n",
      "      it any thribsest on the\n",
      "      the Mrles, in the susitered me\n",
      "      of hould incharning myshated pright\n",
      "      dnep the my the supan\n",
      "      but seade of genty was a, sot that expetsed afted, tood wiuth with a\n",
      "      rechaishand fater I\n",
      "\n",
      "0.5128205128205128 0.27585493844353953 0.015053246378263604\n"
     ]
    }
   ],
   "source": [
    "\n",
    "latest_checkpoint_file = get_latest_checkpoint_file(modelpathname)\n",
    "m = build_model(rnn_units=rnn_units, batch_size=1, nr_lstm_layers=nr_layers, bn=use_bn, bpe_weights=weights1)\n",
    "m.load_weights(latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([1, None]))\n",
    "\n",
    "inputt = ngram_validation_text[1000:1200]\n",
    "validation_text = ngram_validation_text\n",
    "gen_text = generate_text(model=m, start_string=inputt, text_size=1000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=1, p=None)\n",
    "nmax = 2\n",
    "fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=nmax)\n",
    "repetition_score2 = measure_diversity(text_generated=gen_text, n_max=nmax)\n",
    "\n",
    "print(gen_text)\n",
    "print(fraction_correct_words, bleu_score2, repetition_score2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotbool = False\n",
    "outputfile_bool = True\n",
    "\n",
    "# Find latest checkpoint file, because tf.train.latest_checkpoint(dir) doesn't work for some reason\n",
    "modelpathname = 'v12_2'\n",
    "checkpoint_dir = os.path.join(os.path.join(os.path.join(os.getcwd(),\"tmp\"), modelpathname), \"training_checkpoints\")\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "eta = 0.01 # Best = 0.01\n",
    "batch_size = 50 # Best = 25\n",
    "seq_length = 50 # Best = 125\n",
    "num_ns = 4\n",
    "#rint(os.path)3\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "    if e>latest_epoch:\n",
    "        latest_epoch = e\n",
    "        latest_checkpoint_file = file\n",
    "print(latest_checkpoint_file)\n",
    "#latest_checkpoint_file = \"ckpt_01.hdf5\"\n",
    "print(ind_to_char)\n",
    "m = build_model(rnn_units, 1)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([1, None]))\n",
    "\n",
    "nucleus_probability = None\n",
    "temp = 1.0\n",
    "gen_text = generate_text(model=m, start_string=\"the\", text_size=2000, char_to_ind=char_to_ind, ind_to_char=ind_to_char, temp=temp, p=nucleus_probability,word_level=True)\n",
    "\n",
    "\n",
    "if plotbool:\n",
    "    d = get_n_grams(gen_text, 1)\n",
    "    fig = px.line(history.history['loss'], title='Loss over epochs', width=600)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_xaxes(title_text=\"Iteration step, in multiples of 10k\")\n",
    "    fig.update_yaxes(title_text=\"smoothed loss\")\n",
    "    fig.show()\n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "# fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=2)\n",
    "# _, bleu_score4 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=4)\n",
    "# repetition_score2 = measure_diversity(text_generated=gen_text, n_max=2)\n",
    "# repetition_score4 = measure_diversity(text_generated=gen_text, n_max=4)\n",
    "\n",
    "# output_str = gen_text + \"\\n ------------ \\n loss function: {} \\n fraction of correctly spelled words: {} \\n Bleu score2: {}, Repetition score2: {} \"\\\n",
    "#     \"\\n Bleu score4: {}, Repetition score4: {} \".format(history.history['loss'], fraction_correct_words, bleu_score2, repetition_score2, bleu_score4, repetition_score4)\n",
    "# output_str += \"\\n settings: batch_size, seq_length, eta, rnn_units, n_epochs = {}, {}, {}, {}, {}\".format(batch_size, seq_length, eta, rnn_units, n_epochs)\n",
    "print(gen_text)\n",
    "\n",
    "\n",
    "#Create output file\n",
    "output_path = os.path.join(os.path.join(os.path.join(os.getcwd(), \"tmp\"), modelpathname), \"text_temp15.txt\")\n",
    "if outputfile_bool:\n",
    "    with open(output_path, \"w\",encoding=\"utf-8-sig\") as file:\n",
    "        file.write(gen_text)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
