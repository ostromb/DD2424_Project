{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2424 Project in Deep Learning in Data Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filename = '../Dataset/Training/edgar.txt'\n",
    "book_data = np.array(load_data(training_data_filename))\n",
    "book_chars = np.unique(book_data)\n",
    "\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "m = 100\n",
    "eta = 0.1\n",
    "seq_length = 25\n",
    "sig = 0.01\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m, k, eta, seq_length, sig)\n",
    "h0 = np.zeros((m, 1))\n",
    "max_iter = 200000\n",
    "epochs = 10\n",
    "smoothloss_list = []\n",
    "loss_list = []\n",
    "iteration = 0\n",
    "smoothloss = 0\n",
    "sentences = []\n",
    "for i in range(epochs):\n",
    "    rnn.hprev = np.zeros((m, 1))\n",
    "    for e in range(0, book_data.shape[0]-seq_length-1, seq_length):\n",
    "        X_chars = book_data[e:e+seq_length]\n",
    "        Y_chars = book_data[e+1:e+seq_length+1]\n",
    "        X = one_hot_encoding(X_chars, char_to_ind, k)\n",
    "        Y = one_hot_encoding(Y_chars, char_to_ind, k)\n",
    "        loss = rnn.adagrad(X, Y, h0, iteration)\n",
    "        if smoothloss == 0:\n",
    "            smoothloss = loss\n",
    "        smoothloss = 0.999*smoothloss + 0.001*loss\n",
    "     \n",
    "        if iteration % 10000 == 1:\n",
    "            print('Iteration: {}, Loss: {} '.format(iteration, smoothloss))\n",
    "            y = rnn.synthetize(rnn.hprev, X[:, 0], 200)\n",
    "            sentence = one_hot_decoding(y, ind_to_char)\n",
    "            print(sentence + \"\\n\")\n",
    "            #sentences.append(sentence)\n",
    "            smoothloss_list.append(smoothloss)\n",
    "            loss_list.append(loss)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration>max_iter:\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(training_data_filename,encoding='cp850',mode='r') as file:\n",
    "    validation_text = file.read()\n",
    "start_char = \"T\"\n",
    "start_char_onehot = one_hot_encoding(start_char, char_to_ind, k)\n",
    "generated_text_vanilla_onehot = rnn.synthetize(rnn.hprev, start_char_onehot, 1000)\n",
    "generated_text_vanilla = start_char + one_hot_decoding(generated_text_vanilla_onehot, ind_to_char)\n",
    "print(generated_text_vanilla)\n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "nmax = 4\n",
    "fraction_correct_words, bleu_score = measure_bleu(text_generated=generated_text_vanilla, text_val=validation_text, n_max=nmax)\n",
    "repetition_score = measure_diversity(text_generated=generated_text_vanilla, n_max=nmax)\n",
    "print(\"\\n loss function\", loss_list)\n",
    "print(\"\\n fraction of correctly spelled words: {} \\n Bleu score: {} \\n Repetition score: {}\".format(fraction_correct_words, bleu_score, repetition_score))\n",
    "\n",
    "\n",
    "\n",
    "fig = px.line(smoothloss_list, title='Smoothed loss over epochs', width=600)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_yaxes(title_text=\"smoothed loss\")\n",
    "fig.update_xaxes(title_text=\"iteration step, in multiples of 10k\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data_filename = '../Dataset/Training/edgar.txt'\n",
    "book_data = np.array(load_data(training_data_filename,remove_footnotes=False))\n",
    "with open(training_data_filename,encoding='cp850',mode='r') as f:\n",
    "   words = f.read().split()\n",
    "with open(training_data_filename,encoding='cp850',mode='r') as file:\n",
    "    validation_text = file.read()\n",
    "\n",
    "book_chars = np.unique(book_data)\n",
    "char_to_ind = {ch:i for i,ch in enumerate(book_chars)}\n",
    "ind_to_char = {i:ch for i,ch in enumerate(book_chars)}\n",
    "k = book_chars.shape[0]\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 25 # [25, 75, 125]\n",
    "seq_length = 25 # [25, 75, 125]\n",
    "learningrate = 0.1 # [0.1, 0.01, 0.001]\n",
    "book_data_ind = np.array([char_to_ind[c] for c in book_data])\n",
    "\n",
    "# Split data into sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(book_data_ind)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(data):\n",
    "    input = data[:-1]\n",
    "    target = data[1:]\n",
    "    return input, target\n",
    "\n",
    "# Split data into X, Y\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# This organizes the data into groups of sequences. batch_size denotes the number of sequences in a batch, and seq_length denotes the number of characters in a sequence.\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelpathname = 'v6_1'\n",
    "#def loss(Y_true,Y_pred):\n",
    "#    return tf.keras.losses.SparseCategoricalCrossentropy(from)\n",
    "\n",
    "\n",
    "# Length of the vocabulary in chars.\n",
    "vocab_size = len(book_chars)\n",
    "\n",
    "# The embedding dimension.\n",
    "embedding_dim = 256 \n",
    "\n",
    "# Number of RNN units.\n",
    "u = [50, 100, 200, 500, 1000]\n",
    "rnn_units = 200\n",
    "n_epochs = 5\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    m = tf.keras.Sequential()\n",
    "    m.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    m.add(tf.keras.layers.BatchNormalization(synchronized=True))\n",
    "    m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    m.add(tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer=tf.keras.initializers.GlorotNormal()))\n",
    "    m.add(tf.keras.layers.Dense(vocab_size))\n",
    "    #m.summary()\n",
    "    return m\n",
    "\n",
    "m = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "\n",
    "# try the model\n",
    "for i_ex, t_ex in dataset.take(1):\n",
    "    example_pred = m(i_ex)  # this step builds the model\n",
    "    sampled_indices = tf.random.categorical(logits=example_pred[0], num_samples=1)\n",
    "    sample_ind_1d = tf.squeeze(input=sampled_indices, axis=-1).numpy()\n",
    "print('Input:\\n', repr(''.join([ind_to_char[c] for c in i_ex.numpy()[0]])))\n",
    "print('Next char prediction:\\n', repr(''.join([ind_to_char[c] for c in sample_ind_1d])))  \n",
    "\n",
    "# Directory where the checkpoints will be saved.\n",
    "current_dir_path = os.getcwd()\n",
    "checkpoint_dir = current_dir_path + \"\\\\tmp\\\\\" + modelpathname+ \"\\\\training_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch:02d}.hdf5')\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True, \n",
    "    save_best_onl =True\n",
    ")\n",
    "\n",
    "\n",
    "# Specify update rule and compile model\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=learningrate)\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "m.compile(optimizer=adam_opt, loss=loss_func)\n",
    "\n",
    "# train\n",
    "history = m.fit(x=dataset, epochs=n_epochs, callbacks=[checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint file, because tf.train.latest_checkpoint(dir) doesn't work for some reason\n",
    "latest_epoch = 0\n",
    "latest_checkpoint_file = \"\"\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    e = int(file.split(\"_\")[1].split(\".\")[0])\n",
    "    if e>latest_epoch:\n",
    "        latest_epoch = e\n",
    "        latest_checkpoint_file = file\n",
    "print(latest_checkpoint_file)\n",
    "#latest_checkpoint_file = \"ckpt_01.hdf5\"\n",
    "\n",
    "simplified_batch_size = 1\n",
    "m = build_model(vocab_size, embedding_dim, rnn_units, simplified_batch_size)\n",
    "m.load_weights(checkpoint_dir + \"/\" + latest_checkpoint_file)\n",
    "m.build(tf.TensorShape([simplified_batch_size, None]))\n",
    "\n",
    "gen_text = generate_text(m, \"The \", 1000, char_to_ind, ind_to_char)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history['loss'])\n",
    "d = get_n_grams(gen_text, 1)\n",
    "fig = px.line(history.history['loss'], title='Loss over epochs', width=600)\n",
    "#fig = px.scatter(history.history['loss'], title='Loss over epochs')\n",
    "fig.show()\n",
    "\n",
    "correctwords = 0\n",
    "for word in d: \n",
    "    if word in words: \n",
    "        #print(word)\n",
    "        correctwords += 1\n",
    "print(gen_text)\n",
    "\n",
    "\n",
    "print(\"\\n Correct % words: {}\".format(correctwords/len(d)))\n",
    "\n",
    "# Calculate performance metrics for generated text\n",
    "fraction_correct_words, bleu_score2 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=2)\n",
    "fraction_correct_words, bleu_score4 = measure_bleu(text_generated=gen_text, text_val=validation_text, n_max=4)\n",
    "repetition_score2 = measure_diversity(text_generated=gen_text, n_max=2)\n",
    "repetition_score4 = measure_diversity(text_generated=gen_text, n_max=4)\n",
    "print(\"fraction of correctly spelled words: {} \\n Bleu score2: {} \\n Repetition score2: {}\".format(fraction_correct_words, bleu_score2, repetition_score2))\n",
    "print(\"\\n Bleu score4: {} \\n Repetition score4: {}\".format(bleu_score4, repetition_score4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
